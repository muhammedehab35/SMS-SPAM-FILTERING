{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521c6039",
   "metadata": {},
   "source": [
    "# EMAIL SMS:SPAM FILTERING\n",
    "## le but est de cree un model un utilisant le taritement des languages naturels (TAL ou NLP) \n",
    "## le travaille consist a classe les emails en mail interessant ham ou des mail inutil spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "70fbaec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk as nltk\n",
    "import neattext as nt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62cf6a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8babb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Muhammed\n",
      "[nltk_data]     ehab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  # est tres important pour utiliser la tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f67865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARLSTem',\n",
       " 'ARLSTem2',\n",
       " 'AbstractLazySequence',\n",
       " 'AffixTagger',\n",
       " 'AlignedSent',\n",
       " 'Alignment',\n",
       " 'AnnotationTask',\n",
       " 'ApplicationExpression',\n",
       " 'Assignment',\n",
       " 'BigramAssocMeasures',\n",
       " 'BigramCollocationFinder',\n",
       " 'BigramTagger',\n",
       " 'BinaryMaxentFeatureEncoding',\n",
       " 'BlanklineTokenizer',\n",
       " 'BllipParser',\n",
       " 'BottomUpChartParser',\n",
       " 'BottomUpLeftCornerChartParser',\n",
       " 'BottomUpProbabilisticChartParser',\n",
       " 'Boxer',\n",
       " 'BrillTagger',\n",
       " 'BrillTaggerTrainer',\n",
       " 'CFG',\n",
       " 'CRFTagger',\n",
       " 'CfgReadingCommand',\n",
       " 'ChartParser',\n",
       " 'ChunkParserI',\n",
       " 'ChunkScore',\n",
       " 'Cistem',\n",
       " 'ClassifierBasedPOSTagger',\n",
       " 'ClassifierBasedTagger',\n",
       " 'ClassifierI',\n",
       " 'ConcordanceIndex',\n",
       " 'ConditionalExponentialClassifier',\n",
       " 'ConditionalFreqDist',\n",
       " 'ConditionalProbDist',\n",
       " 'ConditionalProbDistI',\n",
       " 'ConfusionMatrix',\n",
       " 'ContextIndex',\n",
       " 'ContextTagger',\n",
       " 'ContingencyMeasures',\n",
       " 'CoreNLPDependencyParser',\n",
       " 'CoreNLPParser',\n",
       " 'Counter',\n",
       " 'CrossValidationProbDist',\n",
       " 'DRS',\n",
       " 'DecisionTreeClassifier',\n",
       " 'DefaultTagger',\n",
       " 'DependencyEvaluator',\n",
       " 'DependencyGrammar',\n",
       " 'DependencyGraph',\n",
       " 'DependencyProduction',\n",
       " 'DictionaryConditionalProbDist',\n",
       " 'DictionaryProbDist',\n",
       " 'DiscourseTester',\n",
       " 'DrtExpression',\n",
       " 'DrtGlueReadingCommand',\n",
       " 'ELEProbDist',\n",
       " 'EarleyChartParser',\n",
       " 'Expression',\n",
       " 'FStructure',\n",
       " 'FeatDict',\n",
       " 'FeatList',\n",
       " 'FeatStruct',\n",
       " 'FeatStructReader',\n",
       " 'Feature',\n",
       " 'FeatureBottomUpChartParser',\n",
       " 'FeatureBottomUpLeftCornerChartParser',\n",
       " 'FeatureChartParser',\n",
       " 'FeatureEarleyChartParser',\n",
       " 'FeatureIncrementalBottomUpChartParser',\n",
       " 'FeatureIncrementalBottomUpLeftCornerChartParser',\n",
       " 'FeatureIncrementalChartParser',\n",
       " 'FeatureIncrementalTopDownChartParser',\n",
       " 'FeatureTopDownChartParser',\n",
       " 'FreqDist',\n",
       " 'HTTPPasswordMgrWithDefaultRealm',\n",
       " 'HeldoutProbDist',\n",
       " 'HiddenMarkovModelTagger',\n",
       " 'HiddenMarkovModelTrainer',\n",
       " 'HunposTagger',\n",
       " 'IBMModel',\n",
       " 'IBMModel1',\n",
       " 'IBMModel2',\n",
       " 'IBMModel3',\n",
       " 'IBMModel4',\n",
       " 'IBMModel5',\n",
       " 'ISRIStemmer',\n",
       " 'ImmutableMultiParentedTree',\n",
       " 'ImmutableParentedTree',\n",
       " 'ImmutableProbabilisticMixIn',\n",
       " 'ImmutableProbabilisticTree',\n",
       " 'ImmutableTree',\n",
       " 'IncrementalBottomUpChartParser',\n",
       " 'IncrementalBottomUpLeftCornerChartParser',\n",
       " 'IncrementalChartParser',\n",
       " 'IncrementalLeftCornerChartParser',\n",
       " 'IncrementalTopDownChartParser',\n",
       " 'Index',\n",
       " 'InsideChartParser',\n",
       " 'JSONTaggedDecoder',\n",
       " 'JSONTaggedEncoder',\n",
       " 'KneserNeyProbDist',\n",
       " 'LancasterStemmer',\n",
       " 'LaplaceProbDist',\n",
       " 'LazyConcatenation',\n",
       " 'LazyEnumerate',\n",
       " 'LazyIteratorList',\n",
       " 'LazyMap',\n",
       " 'LazySubsequence',\n",
       " 'LazyZip',\n",
       " 'LeftCornerChartParser',\n",
       " 'LegalitySyllableTokenizer',\n",
       " 'LidstoneProbDist',\n",
       " 'LineTokenizer',\n",
       " 'LogicalExpressionException',\n",
       " 'LongestChartParser',\n",
       " 'MLEProbDist',\n",
       " 'MWETokenizer',\n",
       " 'Mace',\n",
       " 'MaceCommand',\n",
       " 'MaltParser',\n",
       " 'MaxentClassifier',\n",
       " 'Model',\n",
       " 'MultiClassifierI',\n",
       " 'MultiParentedTree',\n",
       " 'MutableProbDist',\n",
       " 'NLTKWordTokenizer',\n",
       " 'NaiveBayesClassifier',\n",
       " 'NaiveBayesDependencyScorer',\n",
       " 'NgramAssocMeasures',\n",
       " 'NgramTagger',\n",
       " 'NonprojectiveDependencyParser',\n",
       " 'Nonterminal',\n",
       " 'OrderedDict',\n",
       " 'PCFG',\n",
       " 'Paice',\n",
       " 'ParallelProverBuilder',\n",
       " 'ParallelProverBuilderCommand',\n",
       " 'ParentedTree',\n",
       " 'ParserI',\n",
       " 'PerceptronTagger',\n",
       " 'PhraseTable',\n",
       " 'PorterStemmer',\n",
       " 'PositiveNaiveBayesClassifier',\n",
       " 'ProbDistI',\n",
       " 'ProbabilisticDependencyGrammar',\n",
       " 'ProbabilisticMixIn',\n",
       " 'ProbabilisticNonprojectiveParser',\n",
       " 'ProbabilisticProduction',\n",
       " 'ProbabilisticProjectiveDependencyParser',\n",
       " 'ProbabilisticTree',\n",
       " 'Production',\n",
       " 'ProjectiveDependencyParser',\n",
       " 'Prover9',\n",
       " 'Prover9Command',\n",
       " 'ProxyBasicAuthHandler',\n",
       " 'ProxyDigestAuthHandler',\n",
       " 'ProxyHandler',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'QuadgramAssocMeasures',\n",
       " 'QuadgramCollocationFinder',\n",
       " 'RSLPStemmer',\n",
       " 'RTEFeatureExtractor',\n",
       " 'RUS_PICKLE',\n",
       " 'RandomChartParser',\n",
       " 'RangeFeature',\n",
       " 'ReadingCommand',\n",
       " 'RecursiveDescentParser',\n",
       " 'RegexpChunkParser',\n",
       " 'RegexpParser',\n",
       " 'RegexpStemmer',\n",
       " 'RegexpTagger',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'ResolutionProver',\n",
       " 'ResolutionProverCommand',\n",
       " 'SExprTokenizer',\n",
       " 'SLASH',\n",
       " 'Senna',\n",
       " 'SennaChunkTagger',\n",
       " 'SennaNERTagger',\n",
       " 'SennaTagger',\n",
       " 'SequentialBackoffTagger',\n",
       " 'ShiftReduceParser',\n",
       " 'SimpleGoodTuringProbDist',\n",
       " 'SklearnClassifier',\n",
       " 'SlashFeature',\n",
       " 'SnowballStemmer',\n",
       " 'SpaceTokenizer',\n",
       " 'StackDecoder',\n",
       " 'StanfordNERTagger',\n",
       " 'StanfordPOSTagger',\n",
       " 'StanfordSegmenter',\n",
       " 'StanfordTagger',\n",
       " 'StemmerI',\n",
       " 'SteppingChartParser',\n",
       " 'SteppingRecursiveDescentParser',\n",
       " 'SteppingShiftReduceParser',\n",
       " 'SyllableTokenizer',\n",
       " 'TYPE',\n",
       " 'TabTokenizer',\n",
       " 'TableauProver',\n",
       " 'TableauProverCommand',\n",
       " 'TaggerI',\n",
       " 'TestGrammar',\n",
       " 'Text',\n",
       " 'TextCat',\n",
       " 'TextCollection',\n",
       " 'TextTilingTokenizer',\n",
       " 'TnT',\n",
       " 'TokenSearcher',\n",
       " 'ToktokTokenizer',\n",
       " 'TopDownChartParser',\n",
       " 'TransitionParser',\n",
       " 'Tree',\n",
       " 'TreePrettyPrinter',\n",
       " 'TreebankWordDetokenizer',\n",
       " 'TreebankWordTokenizer',\n",
       " 'Trie',\n",
       " 'TrigramAssocMeasures',\n",
       " 'TrigramCollocationFinder',\n",
       " 'TrigramTagger',\n",
       " 'TweetTokenizer',\n",
       " 'TypedMaxentFeatureEncoding',\n",
       " 'Undefined',\n",
       " 'UniformProbDist',\n",
       " 'UnigramTagger',\n",
       " 'UnsortedChartParser',\n",
       " 'Valuation',\n",
       " 'Variable',\n",
       " 'ViterbiParser',\n",
       " 'WekaClassifier',\n",
       " 'WhitespaceTokenizer',\n",
       " 'WittenBellProbDist',\n",
       " 'WordNetLemmatizer',\n",
       " 'WordPunctTokenizer',\n",
       " '__author__',\n",
       " '__author_email__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__classifiers__',\n",
       " '__copyright__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__keywords__',\n",
       " '__license__',\n",
       " '__loader__',\n",
       " '__longdescr__',\n",
       " '__maintainer__',\n",
       " '__maintainer_email__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__url__',\n",
       " '__version__',\n",
       " 'accuracy',\n",
       " 'acyclic_branches_depth_first',\n",
       " 'acyclic_breadth_first',\n",
       " 'acyclic_depth_first',\n",
       " 'acyclic_dic2tree',\n",
       " 'add_logs',\n",
       " 'agreement',\n",
       " 'align',\n",
       " 'alignment_error_rate',\n",
       " 'aline',\n",
       " 'api',\n",
       " 'app',\n",
       " 'apply_features',\n",
       " 'approxrand',\n",
       " 'arity',\n",
       " 'arlstem',\n",
       " 'arlstem2',\n",
       " 'association',\n",
       " 'bigrams',\n",
       " 'binary_distance',\n",
       " 'binary_search_file',\n",
       " 'binding_ops',\n",
       " 'bisect',\n",
       " 'blankline_tokenize',\n",
       " 'bleu',\n",
       " 'bleu_score',\n",
       " 'bllip',\n",
       " 'boolean_ops',\n",
       " 'boxer',\n",
       " 'bracket_parse',\n",
       " 'breadth_first',\n",
       " 'brill',\n",
       " 'brill_trainer',\n",
       " 'build_opener',\n",
       " 'call_megam',\n",
       " 'casual',\n",
       " 'casual_tokenize',\n",
       " 'ccg',\n",
       " 'chain',\n",
       " 'chart',\n",
       " 'chat',\n",
       " 'chomsky_normal_form',\n",
       " 'choose',\n",
       " 'chrf',\n",
       " 'chrf_score',\n",
       " 'chunk',\n",
       " 'cistem',\n",
       " 'classify',\n",
       " 'clause',\n",
       " 'clean_html',\n",
       " 'clean_url',\n",
       " 'cluster',\n",
       " 'collapse_unary',\n",
       " 'collections',\n",
       " 'collocations',\n",
       " 'combinations',\n",
       " 'compat',\n",
       " 'config_java',\n",
       " 'config_megam',\n",
       " 'config_weka',\n",
       " 'conflicts',\n",
       " 'confusionmatrix',\n",
       " 'conllstr2tree',\n",
       " 'conlltags2tree',\n",
       " 'corenlp',\n",
       " 'corpus',\n",
       " 'crf',\n",
       " 'custom_distance',\n",
       " 'data',\n",
       " 'decisiontree',\n",
       " 'decorator',\n",
       " 'decorators',\n",
       " 'defaultdict',\n",
       " 'demo',\n",
       " 'dependencygraph',\n",
       " 'deprecated',\n",
       " 'deque',\n",
       " 'destructive',\n",
       " 'discourse',\n",
       " 'distance',\n",
       " 'download',\n",
       " 'download_gui',\n",
       " 'download_shell',\n",
       " 'downloader',\n",
       " 'draw',\n",
       " 'drt',\n",
       " 'earleychart',\n",
       " 'edge_closure',\n",
       " 'edges2dot',\n",
       " 'edit_distance',\n",
       " 'edit_distance_align',\n",
       " 'elementtree_indent',\n",
       " 'entropy',\n",
       " 'equality_preds',\n",
       " 'evaluate',\n",
       " 'evaluate_sents',\n",
       " 'everygrams',\n",
       " 'extract',\n",
       " 'extract_rels',\n",
       " 'extract_test_sentences',\n",
       " 'f_measure',\n",
       " 'featstruct',\n",
       " 'featurechart',\n",
       " 'filestring',\n",
       " 'find',\n",
       " 'flatten',\n",
       " 'fractional_presence',\n",
       " 'gale_church',\n",
       " 'gdfa',\n",
       " 'getproxies',\n",
       " 'ghd',\n",
       " 'gleu',\n",
       " 'gleu_score',\n",
       " 'glue',\n",
       " 'grammar',\n",
       " 'grow_diag_final_and',\n",
       " 'guess_encoding',\n",
       " 'help',\n",
       " 'hmm',\n",
       " 'hunpos',\n",
       " 'ibm1',\n",
       " 'ibm2',\n",
       " 'ibm3',\n",
       " 'ibm4',\n",
       " 'ibm5',\n",
       " 'ibm_model',\n",
       " 'ieerstr2tree',\n",
       " 'in_idle',\n",
       " 'induce_pcfg',\n",
       " 'inference',\n",
       " 'infile',\n",
       " 'inspect',\n",
       " 'install_opener',\n",
       " 'internals',\n",
       " 'interpret_sents',\n",
       " 'interval_distance',\n",
       " 'invert_dict',\n",
       " 'invert_graph',\n",
       " 'is_rel',\n",
       " 'islice',\n",
       " 'isri',\n",
       " 'jaccard_distance',\n",
       " 'json_tags',\n",
       " 'jsontags',\n",
       " 'lancaster',\n",
       " 'lazyimport',\n",
       " 'legality_principle',\n",
       " 'lfg',\n",
       " 'line_tokenize',\n",
       " 'linearlogic',\n",
       " 'lm',\n",
       " 'load',\n",
       " 'load_parser',\n",
       " 'locale',\n",
       " 'log_likelihood',\n",
       " 'logic',\n",
       " 'mace',\n",
       " 'malt',\n",
       " 'map_tag',\n",
       " 'mapping',\n",
       " 'masi_distance',\n",
       " 'maxent',\n",
       " 'megam',\n",
       " 'memoize',\n",
       " 'meteor',\n",
       " 'meteor_score',\n",
       " 'metrics',\n",
       " 'misc',\n",
       " 'mwe',\n",
       " 'naivebayes',\n",
       " 'ne_chunk',\n",
       " 'ne_chunk_sents',\n",
       " 'ngrams',\n",
       " 'nist',\n",
       " 'nist_score',\n",
       " 'nonprojectivedependencyparser',\n",
       " 'nonterminals',\n",
       " 'numpy',\n",
       " 'os',\n",
       " 'pad_sequence',\n",
       " 'paice',\n",
       " 'pairwise',\n",
       " 'parallelize_preprocess',\n",
       " 'parse',\n",
       " 'parse_sents',\n",
       " 'pchart',\n",
       " 'perceptron',\n",
       " 'phrase_based',\n",
       " 'pk',\n",
       " 'porter',\n",
       " 'pos_tag',\n",
       " 'pos_tag_sents',\n",
       " 'positivenaivebayes',\n",
       " 'pprint',\n",
       " 'pr',\n",
       " 'precision',\n",
       " 'presence',\n",
       " 'print_string',\n",
       " 'probability',\n",
       " 'projectivedependencyparser',\n",
       " 'prover9',\n",
       " 'punkt',\n",
       " 'pydoc',\n",
       " 'raise_unorderable_types',\n",
       " 'ranks_from_scores',\n",
       " 'ranks_from_sequence',\n",
       " 're',\n",
       " 're_show',\n",
       " 'read_grammar',\n",
       " 'read_logic',\n",
       " 'read_valuation',\n",
       " 'recall',\n",
       " 'recursivedescent',\n",
       " 'regexp',\n",
       " 'regexp_span_tokenize',\n",
       " 'regexp_tokenize',\n",
       " 'register_tag',\n",
       " 'relextract',\n",
       " 'repp',\n",
       " 'resolution',\n",
       " 'ribes',\n",
       " 'ribes_score',\n",
       " 'root_semrep',\n",
       " 'rslp',\n",
       " 'rte_classifier',\n",
       " 'rte_classify',\n",
       " 'rte_features',\n",
       " 'rtuple',\n",
       " 'scikitlearn',\n",
       " 'scores',\n",
       " 'segmentation',\n",
       " 'sem',\n",
       " 'senna',\n",
       " 'sent_tokenize',\n",
       " 'sequential',\n",
       " 'set2rel',\n",
       " 'set_proxy',\n",
       " 'sexpr',\n",
       " 'sexpr_tokenize',\n",
       " 'shiftreduce',\n",
       " 'simple',\n",
       " 'sinica_parse',\n",
       " 'skipgrams',\n",
       " 'skolemize',\n",
       " 'slice_bounds',\n",
       " 'snowball',\n",
       " 'sonority_sequencing',\n",
       " 'spearman',\n",
       " 'spearman_correlation',\n",
       " 'stack_decoder',\n",
       " 'stanford',\n",
       " 'stanford_segmenter',\n",
       " 'stem',\n",
       " 'str2tuple',\n",
       " 'string_span_tokenize',\n",
       " 'subprocess',\n",
       " 'subsumes',\n",
       " 'sum_logs',\n",
       " 'tableau',\n",
       " 'tadm',\n",
       " 'tag',\n",
       " 'tagset_mapping',\n",
       " 'tagstr2tree',\n",
       " 'tbl',\n",
       " 'tee',\n",
       " 'text',\n",
       " 'textcat',\n",
       " 'texttiling',\n",
       " 'textwrap',\n",
       " 'tkinter',\n",
       " 'tnt',\n",
       " 'tokenize',\n",
       " 'tokenwrap',\n",
       " 'toktok',\n",
       " 'toolbox',\n",
       " 'total_ordering',\n",
       " 'trace',\n",
       " 'transitionparser',\n",
       " 'transitive_closure',\n",
       " 'translate',\n",
       " 'tree',\n",
       " 'tree2conllstr',\n",
       " 'tree2conlltags',\n",
       " 'treebank',\n",
       " 'trigrams',\n",
       " 'tuple2str',\n",
       " 'un_chomsky_normal_form',\n",
       " 'unify',\n",
       " 'unique_list',\n",
       " 'untag',\n",
       " 'unweighted_minimum_spanning_dict',\n",
       " 'unweighted_minimum_spanning_digraph',\n",
       " 'unweighted_minimum_spanning_tree',\n",
       " 'usage',\n",
       " 'util',\n",
       " 'version_file',\n",
       " 'viterbi',\n",
       " 'warnings',\n",
       " 'weka',\n",
       " 'windowdiff',\n",
       " 'word_tokenize',\n",
       " 'wordnet',\n",
       " 'wordpunct_tokenize',\n",
       " 'wsd']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc6a5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('C:\\\\Users\\Muhammed ehab\\\\OneDrive - ESPRIT\\\\Bureau\\\\MO dataset\\\\spam_ham.csv',encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c51063d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>ham</td>\n",
       "      <td>In xam hall boy asked girl Tell me the starting term for dis answer I can den manage on my own A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5259</th>\n",
       "      <td>ham</td>\n",
       "      <td>Can help u swoop by picking u up from wherever ur other birds r meeting if u want.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>ham</td>\n",
       "      <td>Then u ask darren go n pick u lor... But i oso sian tmr haf 2 meet lect...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       v1  \\\n",
       "2121  ham   \n",
       "5259  ham   \n",
       "2822  ham   \n",
       "\n",
       "                                                                                                       v2  \\\n",
       "2121  In xam hall boy asked girl Tell me the starting term for dis answer I can den manage on my own A...   \n",
       "5259                   Can help u swoop by picking u up from wherever ur other birds r meeting if u want.   \n",
       "2822                           Then u ask darren go n pick u lor... But i oso sian tmr haf 2 meet lect...   \n",
       "\n",
       "     Unnamed: 2 Unnamed: 3 Unnamed: 4  \n",
       "2121        NaN        NaN        NaN  \n",
       "5259        NaN        NaN        NaN  \n",
       "2822        NaN        NaN        NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth',100) # afficher les 100 premiere caracteres\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e45c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer les colones unitils\n",
    "df.drop(columns=['Unnamed: 3','Unnamed: 4','Unnamed: 2'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4b09976",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'v1':'type_de_message','v2':'le_message'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aba52f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_de_message</th>\n",
       "      <th>le_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3482</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hello, my love! How goes that day ? I wish your well and fine babe and hope that you find some j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm not driving... Raining! Then i'll get caught at e mrt station lor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2867</th>\n",
       "      <td>ham</td>\n",
       "      <td>Aight, tomorrow around  &amp;lt;#&amp;gt;  it is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4331</th>\n",
       "      <td>ham</td>\n",
       "      <td>ALSO TELL HIM I SAID HAPPY BIRTHDAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4106</th>\n",
       "      <td>spam</td>\n",
       "      <td>HOT LIVE FANTASIES call now 08707500020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 08...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type_de_message  \\\n",
       "3482             ham   \n",
       "1148             ham   \n",
       "2867             ham   \n",
       "4331             ham   \n",
       "4106            spam   \n",
       "\n",
       "                                                                                               le_message  \n",
       "3482  Hello, my love! How goes that day ? I wish your well and fine babe and hope that you find some j...  \n",
       "1148                               I'm not driving... Raining! Then i'll get caught at e mrt station lor.  \n",
       "2867                                                             Aight, tomorrow around  &lt;#&gt;  it is  \n",
       "4331                                                                  ALSO TELL HIM I SAID HAPPY BIRTHDAY  \n",
       "4106  HOT LIVE FANTASIES call now 08707500020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 08...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c075949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f88421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   type_de_message  5572 non-null   object\n",
      " 1   le_message       5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76d9d527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_de_message    0\n",
       "le_message         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()   # on a pas donc des valeurs manquants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ad21439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_de_message\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.type_de_message.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e15bfa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='type_de_message'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.type_de_message.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d208c2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "le_message\n",
       "Sorry, I'll call later                                                                                                                                                 30\n",
       "I cant pick the phone right now. Pls send a message                                                                                                                    12\n",
       "Ok...                                                                                                                                                                  10\n",
       "7 wonders in My WORLD 7th You 6th Ur style 5th Ur smile 4th Ur Personality 3rd Ur Nature 2nd Ur SMS and 1st \\Ur Lovely Friendship\\\"... good morning dear\"               4\n",
       "Say this slowly.? GOD,I LOVE YOU &amp; I NEED YOU,CLEAN MY HEART WITH YOUR BLOOD.Send this to Ten special people &amp; u c miracle tomorrow, do it,pls,pls do it...     4\n",
       "                                                                                                                                                                       ..\n",
       "I gotta collect da car at 6 lei.                                                                                                                                        1\n",
       "No. On the way home. So if not for the long dry spell the season would have been over                                                                                   1\n",
       "Urgent! Please call 09061743811 from landline. Your ABTA complimentary 4* Tenerife Holiday or å£5000 cash await collection SAE T&Cs Box 326 CW25WX 150ppm               1\n",
       "Dear 0776xxxxxxx U've been invited to XCHAT. This is our final attempt to contact u! Txt CHAT to 86688 150p/MsgrcvdHG/Suite342/2Lands/Row/W1J6HL LDN 18yrs              1\n",
       "Rofl. Its true to its name                                                                                                                                              1\n",
       "Name: count, Length: 5169, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.le_message.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5730a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cree une autre copie de notre dataframe\n",
    "df2=df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3038a6a",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58858382",
   "metadata": {},
   "source": [
    "# Spprimer les ponctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "980a38e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string as str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ea69a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.punctuation  # voici les listes de ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7bc60a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pon=str.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "816d9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suprimer_ponctuation(text):\n",
    "    text_filtrer=''\n",
    "    for x in text:\n",
    "        if x not in pon:\n",
    "            text_filtrer+=x\n",
    "    return text_filtrer\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b7d785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"bonjour c'est mo! je suis un etudiane en AI j'aime beacoup ce domaine est ce que vous me partage les meme sentiment?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5ec91e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bonjour cest mo je suis un etudiane en AI jaime beacoup ce domaine est ce que vous me partage les meme sentiment'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suprimer_ponctuation(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b284a7",
   "metadata": {},
   "source": [
    " le but dans le script suivant est cree une nouvelle collone dans notre data_frme qui s'appelle message_filtrer qui contient la colone\n",
    " message filtrer sans ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f6f9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['le_message_filtrer']=df['le_message'].apply(lambda var:suprimer_ponctuation(var.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d08e438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_de_message</th>\n",
       "      <th>le_message</th>\n",
       "      <th>le_message_filtrer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type_de_message  \\\n",
       "0             ham   \n",
       "1             ham   \n",
       "2            spam   \n",
       "\n",
       "                                                                                            le_message  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "\n",
       "                                                                                    le_message_filtrer  \n",
       "0  go until jurong point crazy available only in bugis n great world la e buffet cine there got amo...  \n",
       "1                                                                              ok lar joking wif u oni  \n",
       "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive e...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb1ff98",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e80d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4f07ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  #   regulae expression module pour manipuler les chaines ds caracteres et les texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36f127a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"j'aime\",\n",
       " 'la',\n",
       " 'programation!',\n",
       " 'et',\n",
       " 'bien',\n",
       " 'sur',\n",
       " 'le',\n",
       " '#NLP',\n",
       " 'et',\n",
       " 'la',\n",
       " 'ML,',\n",
       " 'et',\n",
       " 'DL',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"j'aime la programation! et bien sur le #NLP et la ML, et DL .\"\n",
    "token_text=re.split(' ',text) # il va diviser le text selon les espaces\n",
    "token_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20d9e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    token=word_tokenize(text)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f509c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['le_message_filtrer']=df['le_message_filtrer'].apply(lambda x:tokenize(x)) # tokenization du message filtre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4a63056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_de_message</th>\n",
       "      <th>le_message</th>\n",
       "      <th>le_message_filtrer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u. U have won the å£750 Pound prize. 2 claim is eas...</td>\n",
       "      <td>[this, is, the, 2nd, time, we, have, tried, 2, contact, u, u, have, won, the, å£750, pound, priz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>[will, ì, b, going, to, esplanade, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other suggestions?</td>\n",
       "      <td>[pity, was, in, mood, for, that, soany, other, suggestions]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd be interested in buying something else next week ...</td>\n",
       "      <td>[the, guy, did, some, bitching, but, i, acted, like, id, be, interested, in, buying, something, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>[rofl, its, true, to, its, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     type_de_message  \\\n",
       "0                ham   \n",
       "1                ham   \n",
       "2               spam   \n",
       "3                ham   \n",
       "4                ham   \n",
       "...              ...   \n",
       "5567            spam   \n",
       "5568             ham   \n",
       "5569             ham   \n",
       "5570             ham   \n",
       "5571             ham   \n",
       "\n",
       "                                                                                               le_message  \\\n",
       "0     Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                           Ok lar... Joking wif u oni...   \n",
       "2     Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                       U dun say so early hor... U c already then say...   \n",
       "4                                           Nah I don't think he goes to usf, he lives around here though   \n",
       "...                                                                                                   ...   \n",
       "5567  This is the 2nd time we have tried 2 contact u. U have won the å£750 Pound prize. 2 claim is eas...   \n",
       "5568                                                                Will Ì_ b going to esplanade fr home?   \n",
       "5569                                            Pity, * was in mood for that. So...any other suggestions?   \n",
       "5570  The guy did some bitching but I acted like i'd be interested in buying something else next week ...   \n",
       "5571                                                                           Rofl. Its true to its name   \n",
       "\n",
       "                                                                                       le_message_filtrer  \n",
       "0     [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...  \n",
       "1                                                                          [ok, lar, joking, wif, u, oni]  \n",
       "2     [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...  \n",
       "3                                                 [u, dun, say, so, early, hor, u, c, already, then, say]  \n",
       "4                               [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  \n",
       "...                                                                                                   ...  \n",
       "5567  [this, is, the, 2nd, time, we, have, tried, 2, contact, u, u, have, won, the, å£750, pound, priz...  \n",
       "5568                                                         [will, ì, b, going, to, esplanade, fr, home]  \n",
       "5569                                          [pity, was, in, mood, for, that, soany, other, suggestions]  \n",
       "5570  [the, guy, did, some, bitching, but, i, acted, like, id, be, interested, in, buying, something, ...  \n",
       "5571                                                                     [rofl, its, true, to, its, name]  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75a98a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour enlever les crochets du liste\n",
    "def eliminer_letype_liste(colone):\n",
    "    if isinstance(colone,list):\n",
    "        colone=' '.join(colone)\n",
    "    return colone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21f4ae72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_de_message</th>\n",
       "      <th>le_message</th>\n",
       "      <th>le_message_filtrer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2904</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ha. You donÛ÷t know either. I did a a clever but simple thing with pears the other day, perfect...</td>\n",
       "      <td>ha you donû÷t know either i did a a clever but simple thing with pears the other day perfect fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>ham</td>\n",
       "      <td>Lolnice. I went from a fish to ..water.?</td>\n",
       "      <td>lolnice i went from a fish to water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2881</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hi Harish's rent has been transfred to ur Acnt.</td>\n",
       "      <td>hi harishs rent has been transfred to ur acnt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type_de_message  \\\n",
       "2904             ham   \n",
       "503              ham   \n",
       "2881             ham   \n",
       "\n",
       "                                                                                               le_message  \\\n",
       "2904  Ha. You donÛ÷t know either. I did a a clever but simple thing with pears the other day, perfect...   \n",
       "503                                                              Lolnice. I went from a fish to ..water.?   \n",
       "2881                                                      Hi Harish's rent has been transfred to ur Acnt.   \n",
       "\n",
       "                                                                                       le_message_filtrer  \n",
       "2904  ha you donû÷t know either i did a a clever but simple thing with pears the other day perfect fo...  \n",
       "503                                                                   lolnice i went from a fish to water  \n",
       "2881                                                        hi harishs rent has been transfred to ur acnt  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['le_message_filtrer']=df['le_message_filtrer'].apply(lambda x:eliminer_letype_liste(x))\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6fe945",
   "metadata": {},
   "source": [
    "# Rmoving stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b1f6aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Muhammed\n",
      "[nltk_data]     ehab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')  # tres important pour pourrez utiliser les stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "106e7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54df2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_EN=stopwords.words('english')\n",
    "stopword_FR=stopwords.words('french')\n",
    "stopword_AR=stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b4d85b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e429603a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5015702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا',\n",
       " 'أبٌ',\n",
       " 'أخٌ',\n",
       " 'حمٌ',\n",
       " 'فو',\n",
       " 'أنتِ',\n",
       " 'يناير',\n",
       " 'فبراير',\n",
       " 'مارس',\n",
       " 'أبريل',\n",
       " 'مايو',\n",
       " 'يونيو',\n",
       " 'يوليو',\n",
       " 'أغسطس',\n",
       " 'سبتمبر',\n",
       " 'أكتوبر',\n",
       " 'نوفمبر',\n",
       " 'ديسمبر',\n",
       " 'جانفي',\n",
       " 'فيفري',\n",
       " 'مارس',\n",
       " 'أفريل',\n",
       " 'ماي',\n",
       " 'جوان',\n",
       " 'جويلية',\n",
       " 'أوت',\n",
       " 'كانون',\n",
       " 'شباط',\n",
       " 'آذار',\n",
       " 'نيسان',\n",
       " 'أيار',\n",
       " 'حزيران',\n",
       " 'تموز',\n",
       " 'آب',\n",
       " 'أيلول',\n",
       " 'تشرين',\n",
       " 'دولار',\n",
       " 'دينار',\n",
       " 'ريال',\n",
       " 'درهم',\n",
       " 'ليرة',\n",
       " 'جنيه',\n",
       " 'قرش',\n",
       " 'مليم',\n",
       " 'فلس',\n",
       " 'هللة',\n",
       " 'سنتيم',\n",
       " 'يورو',\n",
       " 'ين',\n",
       " 'يوان',\n",
       " 'شيكل',\n",
       " 'واحد',\n",
       " 'اثنان',\n",
       " 'ثلاثة',\n",
       " 'أربعة',\n",
       " 'خمسة',\n",
       " 'ستة',\n",
       " 'سبعة',\n",
       " 'ثمانية',\n",
       " 'تسعة',\n",
       " 'عشرة',\n",
       " 'أحد',\n",
       " 'اثنا',\n",
       " 'اثني',\n",
       " 'إحدى',\n",
       " 'ثلاث',\n",
       " 'أربع',\n",
       " 'خمس',\n",
       " 'ست',\n",
       " 'سبع',\n",
       " 'ثماني',\n",
       " 'تسع',\n",
       " 'عشر',\n",
       " 'ثمان',\n",
       " 'سبت',\n",
       " 'أحد',\n",
       " 'اثنين',\n",
       " 'ثلاثاء',\n",
       " 'أربعاء',\n",
       " 'خميس',\n",
       " 'جمعة',\n",
       " 'أول',\n",
       " 'ثان',\n",
       " 'ثاني',\n",
       " 'ثالث',\n",
       " 'رابع',\n",
       " 'خامس',\n",
       " 'سادس',\n",
       " 'سابع',\n",
       " 'ثامن',\n",
       " 'تاسع',\n",
       " 'عاشر',\n",
       " 'حادي',\n",
       " 'أ',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ك',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ء',\n",
       " 'ى',\n",
       " 'آ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'أ',\n",
       " 'ة',\n",
       " 'ألف',\n",
       " 'باء',\n",
       " 'تاء',\n",
       " 'ثاء',\n",
       " 'جيم',\n",
       " 'حاء',\n",
       " 'خاء',\n",
       " 'دال',\n",
       " 'ذال',\n",
       " 'راء',\n",
       " 'زاي',\n",
       " 'سين',\n",
       " 'شين',\n",
       " 'صاد',\n",
       " 'ضاد',\n",
       " 'طاء',\n",
       " 'ظاء',\n",
       " 'عين',\n",
       " 'غين',\n",
       " 'فاء',\n",
       " 'قاف',\n",
       " 'كاف',\n",
       " 'لام',\n",
       " 'ميم',\n",
       " 'نون',\n",
       " 'هاء',\n",
       " 'واو',\n",
       " 'ياء',\n",
       " 'همزة',\n",
       " 'ي',\n",
       " 'نا',\n",
       " 'ك',\n",
       " 'كن',\n",
       " 'ه',\n",
       " 'إياه',\n",
       " 'إياها',\n",
       " 'إياهما',\n",
       " 'إياهم',\n",
       " 'إياهن',\n",
       " 'إياك',\n",
       " 'إياكما',\n",
       " 'إياكم',\n",
       " 'إياك',\n",
       " 'إياكن',\n",
       " 'إياي',\n",
       " 'إيانا',\n",
       " 'أولالك',\n",
       " 'تانِ',\n",
       " 'تانِك',\n",
       " 'تِه',\n",
       " 'تِي',\n",
       " 'تَيْنِ',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'ذانِ',\n",
       " 'ذِه',\n",
       " 'ذِي',\n",
       " 'ذَيْنِ',\n",
       " 'هَؤلاء',\n",
       " 'هَاتانِ',\n",
       " 'هَاتِه',\n",
       " 'هَاتِي',\n",
       " 'هَاتَيْنِ',\n",
       " 'هَذا',\n",
       " 'هَذانِ',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'هَذَيْنِ',\n",
       " 'الألى',\n",
       " 'الألاء',\n",
       " 'أل',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'ذيت',\n",
       " 'كأيّ',\n",
       " 'كأيّن',\n",
       " 'بضع',\n",
       " 'فلان',\n",
       " 'وا',\n",
       " 'آمينَ',\n",
       " 'آهِ',\n",
       " 'آهٍ',\n",
       " 'آهاً',\n",
       " 'أُفٍّ',\n",
       " 'أُفٍّ',\n",
       " 'أفٍّ',\n",
       " 'أمامك',\n",
       " 'أمامكَ',\n",
       " 'أوّهْ',\n",
       " 'إلَيْكَ',\n",
       " 'إلَيْكَ',\n",
       " 'إليكَ',\n",
       " 'إليكنّ',\n",
       " 'إيهٍ',\n",
       " 'بخٍ',\n",
       " 'بسّ',\n",
       " 'بَسْ',\n",
       " 'بطآن',\n",
       " 'بَلْهَ',\n",
       " 'حاي',\n",
       " 'حَذارِ',\n",
       " 'حيَّ',\n",
       " 'حيَّ',\n",
       " 'دونك',\n",
       " 'رويدك',\n",
       " 'سرعان',\n",
       " 'شتانَ',\n",
       " 'شَتَّانَ',\n",
       " 'صهْ',\n",
       " 'صهٍ',\n",
       " 'طاق',\n",
       " 'طَق',\n",
       " 'عَدَسْ',\n",
       " 'كِخ',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانكم',\n",
       " 'مكانكما',\n",
       " 'مكانكنّ',\n",
       " 'نَخْ',\n",
       " 'هاكَ',\n",
       " 'هَجْ',\n",
       " 'هلم',\n",
       " 'هيّا',\n",
       " 'هَيْهات',\n",
       " 'وا',\n",
       " 'واهاً',\n",
       " 'وراءَك',\n",
       " 'وُشْكَانَ',\n",
       " 'وَيْ',\n",
       " 'يفعلان',\n",
       " 'تفعلان',\n",
       " 'يفعلون',\n",
       " 'تفعلون',\n",
       " 'تفعلين',\n",
       " 'اتخذ',\n",
       " 'ألفى',\n",
       " 'تخذ',\n",
       " 'ترك',\n",
       " 'تعلَّم',\n",
       " 'جعل',\n",
       " 'حجا',\n",
       " 'حبيب',\n",
       " 'خال',\n",
       " 'حسب',\n",
       " 'خال',\n",
       " 'درى',\n",
       " 'رأى',\n",
       " 'زعم',\n",
       " 'صبر',\n",
       " 'ظنَّ',\n",
       " 'عدَّ',\n",
       " 'علم',\n",
       " 'غادر',\n",
       " 'ذهب',\n",
       " 'وجد',\n",
       " 'ورد',\n",
       " 'وهب',\n",
       " 'أسكن',\n",
       " 'أطعم',\n",
       " 'أعطى',\n",
       " 'رزق',\n",
       " 'زود',\n",
       " 'سقى',\n",
       " 'كسا',\n",
       " 'أخبر',\n",
       " 'أرى',\n",
       " 'أعلم',\n",
       " 'أنبأ',\n",
       " 'حدَث',\n",
       " 'خبَّر',\n",
       " 'نبَّا',\n",
       " 'أفعل به',\n",
       " 'ما أفعله',\n",
       " 'بئس',\n",
       " 'ساء',\n",
       " 'طالما',\n",
       " 'قلما',\n",
       " 'لات',\n",
       " 'لكنَّ',\n",
       " 'ءَ',\n",
       " 'أجل',\n",
       " 'إذاً',\n",
       " 'أمّا',\n",
       " 'إمّا',\n",
       " 'إنَّ',\n",
       " 'أنًّ',\n",
       " 'أى',\n",
       " 'إى',\n",
       " 'أيا',\n",
       " 'ب',\n",
       " 'ثمَّ',\n",
       " 'جلل',\n",
       " 'جير',\n",
       " 'رُبَّ',\n",
       " 'س',\n",
       " 'علًّ',\n",
       " 'ف',\n",
       " 'كأنّ',\n",
       " 'كلَّا',\n",
       " 'كى',\n",
       " 'ل',\n",
       " 'لات',\n",
       " 'لعلَّ',\n",
       " 'لكنَّ',\n",
       " 'لكنَّ',\n",
       " 'م',\n",
       " 'نَّ',\n",
       " 'هلّا',\n",
       " 'وا',\n",
       " 'أل',\n",
       " 'إلّا',\n",
       " 'ت',\n",
       " 'ك',\n",
       " 'لمّا',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ا',\n",
       " 'ي',\n",
       " 'تجاه',\n",
       " 'تلقاء',\n",
       " 'جميع',\n",
       " 'حسب',\n",
       " 'سبحان',\n",
       " 'شبه',\n",
       " 'لعمر',\n",
       " 'مثل',\n",
       " 'معاذ',\n",
       " 'أبو',\n",
       " 'أخو',\n",
       " 'حمو',\n",
       " 'فو',\n",
       " 'مئة',\n",
       " 'مئتان',\n",
       " 'ثلاثمئة',\n",
       " 'أربعمئة',\n",
       " 'خمسمئة',\n",
       " 'ستمئة',\n",
       " 'سبعمئة',\n",
       " 'ثمنمئة',\n",
       " 'تسعمئة',\n",
       " 'مائة',\n",
       " 'ثلاثمائة',\n",
       " 'أربعمائة',\n",
       " 'خمسمائة',\n",
       " 'ستمائة',\n",
       " 'سبعمائة',\n",
       " 'ثمانمئة',\n",
       " 'تسعمائة',\n",
       " 'عشرون',\n",
       " 'ثلاثون',\n",
       " 'اربعون',\n",
       " 'خمسون',\n",
       " 'ستون',\n",
       " 'سبعون',\n",
       " 'ثمانون',\n",
       " 'تسعون',\n",
       " 'عشرين',\n",
       " 'ثلاثين',\n",
       " 'اربعين',\n",
       " 'خمسين',\n",
       " 'ستين',\n",
       " 'سبعين',\n",
       " 'ثمانين',\n",
       " 'تسعين',\n",
       " 'بضع',\n",
       " 'نيف',\n",
       " 'أجمع',\n",
       " 'جميع',\n",
       " 'عامة',\n",
       " 'عين',\n",
       " 'نفس',\n",
       " 'لا سيما',\n",
       " 'أصلا',\n",
       " 'أهلا',\n",
       " 'أيضا',\n",
       " 'بؤسا',\n",
       " 'بعدا',\n",
       " 'بغتة',\n",
       " 'تعسا',\n",
       " 'حقا',\n",
       " 'حمدا',\n",
       " 'خلافا',\n",
       " 'خاصة',\n",
       " 'دواليك',\n",
       " 'سحقا',\n",
       " 'سرا',\n",
       " 'سمعا',\n",
       " 'صبرا',\n",
       " 'صدقا',\n",
       " 'صراحة',\n",
       " 'طرا',\n",
       " 'عجبا',\n",
       " 'عيانا',\n",
       " 'غالبا',\n",
       " 'فرادى',\n",
       " 'فضلا',\n",
       " 'قاطبة',\n",
       " 'كثيرا',\n",
       " 'لبيك',\n",
       " 'معاذ',\n",
       " 'أبدا',\n",
       " 'إزاء',\n",
       " 'أصلا',\n",
       " 'الآن',\n",
       " 'أمد',\n",
       " 'أمس',\n",
       " 'آنفا',\n",
       " 'آناء',\n",
       " 'أنّى',\n",
       " 'أول',\n",
       " 'أيّان',\n",
       " 'تارة',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'حقا',\n",
       " 'صباح',\n",
       " 'مساء',\n",
       " 'ضحوة',\n",
       " 'عوض',\n",
       " 'غدا',\n",
       " 'غداة',\n",
       " 'قطّ',\n",
       " 'كلّما',\n",
       " 'لدن',\n",
       " 'لمّا',\n",
       " 'مرّة',\n",
       " 'قبل',\n",
       " 'خلف',\n",
       " 'أمام',\n",
       " 'فوق',\n",
       " 'تحت',\n",
       " 'يمين',\n",
       " 'شمال',\n",
       " 'ارتدّ',\n",
       " 'استحال',\n",
       " 'أصبح',\n",
       " 'أضحى',\n",
       " 'آض',\n",
       " 'أمسى',\n",
       " 'انقلب',\n",
       " 'بات',\n",
       " 'تبدّل',\n",
       " 'تحوّل',\n",
       " 'حار',\n",
       " 'رجع',\n",
       " 'راح',\n",
       " 'صار',\n",
       " 'ظلّ',\n",
       " 'عاد',\n",
       " 'غدا',\n",
       " 'كان',\n",
       " 'ما انفك',\n",
       " 'ما برح',\n",
       " 'مادام',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ابتدأ',\n",
       " 'أخذ',\n",
       " 'اخلولق',\n",
       " 'أقبل',\n",
       " 'انبرى',\n",
       " 'أنشأ',\n",
       " 'أوشك',\n",
       " 'جعل',\n",
       " 'حرى',\n",
       " 'شرع',\n",
       " 'طفق',\n",
       " 'علق',\n",
       " 'قام',\n",
       " 'كرب',\n",
       " 'كاد',\n",
       " 'هبّ']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07bc9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# il faut faire attentiin en manipulant les stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc4887f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suprimer_stop_words(text):\n",
    "    text_without_stop_words=''\n",
    "    for x in text:\n",
    "        if x not in stopword_FR:\n",
    "            text_without_stop_words+=x\n",
    "    return text_without_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db6a568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"bonjour a tous je m'appelle mo ehab depuis j'ai 24 ans je vais chaque jours a ESPRIT \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53df3b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"boour a ou e 'appee o ehab epui 'ai 24 a e vai haque our a ESPRIT \""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suprimer_stop_words(text1)  # comme vous voyez le phrase est devenue sans aucune sens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84a8e95c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# essayons un peux d'ameliorer notre liste des stopwords\n",
    "def eliminer_les_seuls_letres(stop_words):\n",
    "    nouv_list_pon=[]\n",
    "    for x in stop_words:\n",
    "        if len(x)>1:\n",
    "            nouv_list_pon.append(x)\n",
    "    return nouv_list_pon      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cacdd784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent',\n",
       " ' a ']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_fr=eliminer_les_seuls_letres(stopword_FR)\n",
    "stop_fr.append(' a ')   # sera tres util a ajouter a dans notre list filtre\n",
    "stop_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dbd9573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefinissons maintenant la fonction suprimer_stop_words\n",
    "def suprimer_stop_words_fr(text):\n",
    "    text_sans_stop_words=[]\n",
    "    nouv_text=text.split()\n",
    "    for x in nouv_text:\n",
    "        if x not in stop_fr:\n",
    "            text_sans_stop_words.append(x)\n",
    "    var=' '.join(text_sans_stop_words)   \n",
    "    # join utiliser pour conactener les element de types string d'une listeen une chaine \n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "163b5474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bonjour a tous m'appelle mo ehab depuis j'ai 24 ans j'etud a esprit j'aime universite\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1=\"bonjour a tous je m'appelle mo ehab depuis j'ai 24 ans j'etud a esprit et j'aime mon universite  \"\n",
    "suprimer_stop_words_fr(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ebd665cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stop_words(text):\n",
    "    text_without_stop_words=''\n",
    "    for x in text:\n",
    "        if x not in stopword_EN:\n",
    "            text_without_stop_words+=x\n",
    "    return text_without_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "877249ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2=\"hello my name is mo my level in english is not up for that i try every day to learn new things\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2ec94d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hell  ne    level n englh  n up fr h  r ever   lern new hng'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_stop_words(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09fd2e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'll',\n",
       " 're',\n",
       " 've',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_en=eliminer_les_seuls_letres(stopword_EN)\n",
    "stop_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67013fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# la meme chose est rencontre aussi avec anglais\n",
    "def delete_stop_words2(text):\n",
    "    if isinstance(text, object):\n",
    "        nouv_text=text.split()   # est tres util a diviser le text en mots en cas le text est une chaine\n",
    "    else:\n",
    "        nouv_text=text\n",
    "    text_without_stop_words=''\n",
    "    for x in nouv_text:\n",
    "        if x not in stop_en:\n",
    "            text_without_stop_words+=' '+ x\n",
    "    return text_without_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "748a28af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hello name mo level english i try every day learn new things'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_stop_words2(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b801e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_de_message</th>\n",
       "      <th>le_message</th>\n",
       "      <th>le_message_filtrer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hey... Why dont we just go watch x men and have lunch... Haha</td>\n",
       "      <td>hey why dont we just go watch x men and have lunch haha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>ham</td>\n",
       "      <td>How are you doing. How's the queen. Are you going for the royal wedding</td>\n",
       "      <td>how are you doing hows the queen are you going for the royal wedding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type_de_message  \\\n",
       "321              ham   \n",
       "4174             ham   \n",
       "\n",
       "                                                                   le_message  \\\n",
       "321            Hey... Why dont we just go watch x men and have lunch... Haha    \n",
       "4174  How are you doing. How's the queen. Are you going for the royal wedding   \n",
       "\n",
       "                                                        le_message_filtrer  \n",
       "321                hey why dont we just go watch x men and have lunch haha  \n",
       "4174  how are you doing hows the queen are you going for the royal wedding  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "929bb3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_de_message</th>\n",
       "      <th>le_message</th>\n",
       "      <th>le_message_filtrer</th>\n",
       "      <th>le_message_sans_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>spam</td>\n",
       "      <td>Spook up your mob with a Halloween collection of a logo &amp; pic message plus a free eerie tone, tx...</td>\n",
       "      <td>spook up your mob with a halloween collection of a logo pic message plus a free eerie tone txt c...</td>\n",
       "      <td>spook mob a halloween collection a logo pic message plus a free eerie tone txt card spook 8007 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>ham</td>\n",
       "      <td>Today is \\song dedicated day..\\\" Which song will u dedicate for me? Send this to all ur valuable...</td>\n",
       "      <td>today is song dedicated day which song will u dedicate for me send this to all ur valuable frnds...</td>\n",
       "      <td>today song dedicated day song u dedicate send ur valuable frnds first rply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5355</th>\n",
       "      <td>ham</td>\n",
       "      <td>Living is very simple.. Loving is also simple.. Laughing is too simple.. Winning is tooo simple....</td>\n",
       "      <td>living is very simple loving is also simple laughing is too simple winning is tooo simple but be...</td>\n",
       "      <td>living simple loving also simple laughing simple winning tooo simple simple difficult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type_de_message  \\\n",
       "2359            spam   \n",
       "66               ham   \n",
       "5355             ham   \n",
       "\n",
       "                                                                                               le_message  \\\n",
       "2359  Spook up your mob with a Halloween collection of a logo & pic message plus a free eerie tone, tx...   \n",
       "66    Today is \\song dedicated day..\\\" Which song will u dedicate for me? Send this to all ur valuable...   \n",
       "5355  Living is very simple.. Loving is also simple.. Laughing is too simple.. Winning is tooo simple....   \n",
       "\n",
       "                                                                                       le_message_filtrer  \\\n",
       "2359  spook up your mob with a halloween collection of a logo pic message plus a free eerie tone txt c...   \n",
       "66    today is song dedicated day which song will u dedicate for me send this to all ur valuable frnds...   \n",
       "5355  living is very simple loving is also simple laughing is too simple winning is tooo simple but be...   \n",
       "\n",
       "                                                                                le_message_sans_stopwords  \n",
       "2359   spook mob a halloween collection a logo pic message plus a free eerie tone txt card spook 8007 ...  \n",
       "66                             today song dedicated day song u dedicate send ur valuable frnds first rply  \n",
       "5355                living simple loving also simple laughing simple winning tooo simple simple difficult  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['le_message_sans_stopwords']=df['le_message_filtrer'].apply(lambda var:delete_stop_words2(var))\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33670a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_de_message</th>\n",
       "      <th>le_message</th>\n",
       "      <th>le_message_filtrer</th>\n",
       "      <th>le_message_sans_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>spam</td>\n",
       "      <td>You are a winner U have been specially selected 2 receive å£1000 cash or a 4* holiday (flights i...</td>\n",
       "      <td>you are a winner u have been specially selected 2 receive å£1000 cash or a 4 holiday flights inc...</td>\n",
       "      <td>a winner u specially selected 2 receive å£1000 cash a 4 holiday flights inc speak a live operat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>ham</td>\n",
       "      <td>Been up to ne thing interesting. Did you have a good birthday? When are u wrking nxt? I started ...</td>\n",
       "      <td>been up to ne thing interesting did you have a good birthday when are u wrking nxt i started uni...</td>\n",
       "      <td>ne thing interesting a good birthday u wrking nxt i started uni today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364</th>\n",
       "      <td>spam</td>\n",
       "      <td>Call 09095350301 and send our girls into erotic ecstacy. Just 60p/min. To stop texts call 087124...</td>\n",
       "      <td>call 09095350301 and send our girls into erotic ecstacy just 60pmin to stop texts call 087124603...</td>\n",
       "      <td>call 09095350301 send girls erotic ecstacy 60pmin stop texts call 08712460324 nat rate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type_de_message  \\\n",
       "1224            spam   \n",
       "1099             ham   \n",
       "5364            spam   \n",
       "\n",
       "                                                                                               le_message  \\\n",
       "1224  You are a winner U have been specially selected 2 receive å£1000 cash or a 4* holiday (flights i...   \n",
       "1099  Been up to ne thing interesting. Did you have a good birthday? When are u wrking nxt? I started ...   \n",
       "5364  Call 09095350301 and send our girls into erotic ecstacy. Just 60p/min. To stop texts call 087124...   \n",
       "\n",
       "                                                                                       le_message_filtrer  \\\n",
       "1224  you are a winner u have been specially selected 2 receive å£1000 cash or a 4 holiday flights inc...   \n",
       "1099  been up to ne thing interesting did you have a good birthday when are u wrking nxt i started uni...   \n",
       "5364  call 09095350301 and send our girls into erotic ecstacy just 60pmin to stop texts call 087124603...   \n",
       "\n",
       "                                                                                le_message_sans_stopwords  \n",
       "1224   a winner u specially selected 2 receive å£1000 cash a 4 holiday flights inc speak a live operat...  \n",
       "1099                                ne thing interesting a good birthday u wrking nxt i started uni today  \n",
       "5364               call 09095350301 send girls erotic ecstacy 60pmin stop texts call 08712460324 nat rate  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows',100)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244999a3",
   "metadata": {},
   "source": [
    "# STEMING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b4fdfe",
   "metadata": {},
   "source": [
    "# souvent nous rncontrons deux types de problemes tres courant avec le steming\n",
    "under steming\n",
    "over steming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a984c",
   "metadata": {},
   "source": [
    "## le stemming n'utilise pas un dictionnaire  il utilise un algorithm predefinie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e61c0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=nltk.PorterStemmer()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "47a8a465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MARTIN_EXTENSIONS',\n",
       " 'NLTK_EXTENSIONS',\n",
       " 'ORIGINAL_ALGORITHM',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_apply_rule_list',\n",
       " '_contains_vowel',\n",
       " '_ends_cvc',\n",
       " '_ends_double_consonant',\n",
       " '_has_positive_measure',\n",
       " '_is_consonant',\n",
       " '_measure',\n",
       " '_replace_suffix',\n",
       " '_step1a',\n",
       " '_step1b',\n",
       " '_step1c',\n",
       " '_step2',\n",
       " '_step3',\n",
       " '_step4',\n",
       " '_step5a',\n",
       " '_step5b',\n",
       " 'mode',\n",
       " 'pool',\n",
       " 'stem',\n",
       " 'vowels']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ps)  # pour voir les fonctions qu'il contient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "74398c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "player\n",
      "play\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('play'))\n",
    "print(ps.stem('player'))\n",
    "print(ps.stem('playing'))\n",
    "print(ps.stem('played'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00aae5",
   "metadata": {},
   "source": [
    "## exemple du oversteming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64af997d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "univers\n",
      "univers\n",
      "univers\n"
     ]
    }
   ],
   "source": [
    "# il rend des mots qui ont differnt sens a une seul origine\n",
    "print(ps.stem('universe'))\n",
    "print(ps.stem('universal'))\n",
    "print(ps.stem('university'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c710f7",
   "metadata": {},
   "source": [
    "## exemple du understeming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38226225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alumnu\n",
      "alumni\n",
      "alumna\n"
     ]
    }
   ],
   "source": [
    "# il rend des mots qui ont le meme sens a des differents origines\n",
    "print(ps.stem('alumnus'))\n",
    "print(ps.stem('alumni'))\n",
    "print(ps.stem('alumnae'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "59c5bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steaming(text):\n",
    "    text_steam=[]\n",
    "    text=text.split()\n",
    "    for x in text:\n",
    "        text_steam.append(ps.stem(x))\n",
    "    text_steam= ' '.join(text_steam)\n",
    "    return text_steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e86da434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message_steming']=df['le_message_sans_stopwords'].apply(lambda x:steaming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c972e38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_de_message</th>\n",
       "      <th>le_message</th>\n",
       "      <th>le_message_filtrer</th>\n",
       "      <th>le_message_sans_stopwords</th>\n",
       "      <th>message_steming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4378</th>\n",
       "      <td>ham</td>\n",
       "      <td>How are you. Just checking up on you</td>\n",
       "      <td>how are you just checking up on you</td>\n",
       "      <td>checking</td>\n",
       "      <td>check</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hi' Test on  &amp;lt;#&amp;gt; rd ....</td>\n",
       "      <td>hi test on ltgt rd</td>\n",
       "      <td>hi test ltgt rd</td>\n",
       "      <td>hi test ltgt rd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3544</th>\n",
       "      <td>ham</td>\n",
       "      <td>Thank You meet you monday</td>\n",
       "      <td>thank you meet you monday</td>\n",
       "      <td>thank meet monday</td>\n",
       "      <td>thank meet monday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type_de_message                            le_message  \\\n",
       "4378             ham  How are you. Just checking up on you   \n",
       "2930             ham        Hi' Test on  &lt;#&gt; rd ....   \n",
       "3544             ham             Thank You meet you monday   \n",
       "\n",
       "                       le_message_filtrer le_message_sans_stopwords  \\\n",
       "4378  how are you just checking up on you                  checking   \n",
       "2930                   hi test on ltgt rd           hi test ltgt rd   \n",
       "3544            thank you meet you monday         thank meet monday   \n",
       "\n",
       "        message_steming  \n",
       "4378              check  \n",
       "2930    hi test ltgt rd  \n",
       "3544  thank meet monday  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d3911",
   "metadata": {},
   "source": [
    "# LEMMATIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e048193e",
   "metadata": {},
   "source": [
    "##  par contre de la steming le lemitazition utilise un dictionnaire donc c'est lent mais plus performant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8bc09d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Muhammed\n",
      "[nltk_data]     ehab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61979d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl=nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cab6cdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'lemmatize']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fa7aed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aapliquons sur les meme exemples vu stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "befcf244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universe\n",
      "universal\n",
      "university\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize('universe'))\n",
    "print(wnl.lemmatize('universal'))\n",
    "print(wnl.lemmatize('university'))  \n",
    "# on peut remarque qu'il a pris du  temps car il va consulter son dictionnaire mais le resultats sont parfait "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f915a0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alumnus\n",
      "alumnus\n",
      "alumna\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize('alumnus'))\n",
    "print(wnl.lemmatize('alumni'))\n",
    "print(wnl.lemmatize('alumnae')) \n",
    "# on remarque qu'il a bien donner les 2 premieres mots par contre la 3eme ce qui est explique par l'abscence de ce mot dans le dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8684d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmmatizer(text):\n",
    "    text_lemmatizer=[]\n",
    "    text=text.split()\n",
    "    for x in text:\n",
    "        text_lemmatizer.append(wnl.lemmatize(x))\n",
    "    text_lemmatizer= ' '.join(text_lemmatizer)\n",
    "    return text_lemmatizer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3b560e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message_lemmatizer']=df['le_message_sans_stopwords'].apply( lambda x:lemmmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7697bf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_de_message</th>\n",
       "      <th>le_message</th>\n",
       "      <th>le_message_filtrer</th>\n",
       "      <th>le_message_sans_stopwords</th>\n",
       "      <th>message_steming</th>\n",
       "      <th>message_lemmatizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5152</th>\n",
       "      <td>ham</td>\n",
       "      <td>Idk. I'm sitting here in a stop and shop parking lot right now bawling my eyes out because i fee...</td>\n",
       "      <td>idk im sitting here in a stop and shop parking lot right now bawling my eyes out because i feel ...</td>\n",
       "      <td>idk im sitting a stop shop parking lot right bawling eyes i feel like im a failure everything n...</td>\n",
       "      <td>idk im sit a stop shop park lot right bawl eye i feel like im a failur everyth nobodi want i fee...</td>\n",
       "      <td>idk im sitting a stop shop parking lot right bawling eye i feel like im a failure everything nob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>ham</td>\n",
       "      <td>The beauty of life is in next second.. which hides thousands of secrets. I wish every second wil...</td>\n",
       "      <td>the beauty of life is in next second which hides thousands of secrets i wish every second will b...</td>\n",
       "      <td>beauty life next second hides thousands secrets i wish every second wonderful ur life gud n8</td>\n",
       "      <td>beauti life next second hide thousand secret i wish everi second wonder ur life gud n8</td>\n",
       "      <td>beauty life next second hide thousand secret i wish every second wonderful ur life gud n8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334</th>\n",
       "      <td>ham</td>\n",
       "      <td>Garbage bags, eggs, jam, bread, hannaford wheat chex</td>\n",
       "      <td>garbage bags eggs jam bread hannaford wheat chex</td>\n",
       "      <td>garbage bags eggs jam bread hannaford wheat chex</td>\n",
       "      <td>garbag bag egg jam bread hannaford wheat chex</td>\n",
       "      <td>garbage bag egg jam bread hannaford wheat chex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2806</th>\n",
       "      <td>spam</td>\n",
       "      <td>December only! Had your mobile 11mths+? You are entitled to update to the latest colour camera m...</td>\n",
       "      <td>december only had your mobile 11mths you are entitled to update to the latest colour camera mobi...</td>\n",
       "      <td>december mobile 11mths entitled update latest colour camera mobile free call mobile update co f...</td>\n",
       "      <td>decemb mobil 11mth entitl updat latest colour camera mobil free call mobil updat co free 0800298...</td>\n",
       "      <td>december mobile 11mths entitled update latest colour camera mobile free call mobile update co fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type_de_message  \\\n",
       "5152             ham   \n",
       "4747             ham   \n",
       "5334             ham   \n",
       "2806            spam   \n",
       "\n",
       "                                                                                               le_message  \\\n",
       "5152  Idk. I'm sitting here in a stop and shop parking lot right now bawling my eyes out because i fee...   \n",
       "4747  The beauty of life is in next second.. which hides thousands of secrets. I wish every second wil...   \n",
       "5334                                                 Garbage bags, eggs, jam, bread, hannaford wheat chex   \n",
       "2806  December only! Had your mobile 11mths+? You are entitled to update to the latest colour camera m...   \n",
       "\n",
       "                                                                                       le_message_filtrer  \\\n",
       "5152  idk im sitting here in a stop and shop parking lot right now bawling my eyes out because i feel ...   \n",
       "4747  the beauty of life is in next second which hides thousands of secrets i wish every second will b...   \n",
       "5334                                                     garbage bags eggs jam bread hannaford wheat chex   \n",
       "2806  december only had your mobile 11mths you are entitled to update to the latest colour camera mobi...   \n",
       "\n",
       "                                                                                le_message_sans_stopwords  \\\n",
       "5152   idk im sitting a stop shop parking lot right bawling eyes i feel like im a failure everything n...   \n",
       "4747         beauty life next second hides thousands secrets i wish every second wonderful ur life gud n8   \n",
       "5334                                                     garbage bags eggs jam bread hannaford wheat chex   \n",
       "2806   december mobile 11mths entitled update latest colour camera mobile free call mobile update co f...   \n",
       "\n",
       "                                                                                          message_steming  \\\n",
       "5152  idk im sit a stop shop park lot right bawl eye i feel like im a failur everyth nobodi want i fee...   \n",
       "4747               beauti life next second hide thousand secret i wish everi second wonder ur life gud n8   \n",
       "5334                                                        garbag bag egg jam bread hannaford wheat chex   \n",
       "2806  decemb mobil 11mth entitl updat latest colour camera mobil free call mobil updat co free 0800298...   \n",
       "\n",
       "                                                                                       message_lemmatizer  \n",
       "5152  idk im sitting a stop shop parking lot right bawling eye i feel like im a failure everything nob...  \n",
       "4747            beauty life next second hide thousand secret i wish every second wonderful ur life gud n8  \n",
       "5334                                                       garbage bag egg jam bread hannaford wheat chex  \n",
       "2806  december mobile 11mths entitled update latest colour camera mobile free call mobile update co fr...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "71fadf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_de_message</th>\n",
       "      <th>le_message</th>\n",
       "      <th>message_steming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>ham</td>\n",
       "      <td>Aiyo... U always c our ex one... I dunno abt mei, she haven reply... First time u reply so fast....</td>\n",
       "      <td>aiyo u alway c ex one i dunno abt mei repli first time u repli fast y lucki workin huh got bao u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>ham</td>\n",
       "      <td>Trust me. Even if isn't there, its there.</td>\n",
       "      <td>trust even isnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>ham</td>\n",
       "      <td>I gotta collect da car at 6 lei.</td>\n",
       "      <td>i got ta collect da car 6 lei</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type_de_message  \\\n",
       "559              ham   \n",
       "2076             ham   \n",
       "1889             ham   \n",
       "\n",
       "                                                                                               le_message  \\\n",
       "559   Aiyo... U always c our ex one... I dunno abt mei, she haven reply... First time u reply so fast....   \n",
       "2076                                                            Trust me. Even if isn't there, its there.   \n",
       "1889                                                                     I gotta collect da car at 6 lei.   \n",
       "\n",
       "                                                                                          message_steming  \n",
       "559   aiyo u alway c ex one i dunno abt mei repli first time u repli fast y lucki workin huh got bao u...  \n",
       "2076                                                                                      trust even isnt  \n",
       "1889                                                                        i got ta collect da car 6 lei  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# donc coisissons une seul methode pour filtrer un peu nottre df et je choix le steming\n",
    "df=df[['type_de_message','le_message','message_steming']]\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22dc2d2",
   "metadata": {},
   "source": [
    "# VECTORIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d7356",
   "metadata": {},
   "source": [
    "## LE VECTORIZATION EST UNE TECHNIQUE SERT A CONVERTIR LES TEXTS EN ENSEMBLES DES VALEURS NUMERIQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1bd565f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testons les differnts techniques de vectorization avec la lidte suivant\n",
    "l=['je suis un data scientist','data science','je suis un espritiens','AI','ML est une sous domaine de AI']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c4ad5",
   "metadata": {},
   "source": [
    "## COUNT VECCTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "166c4494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8173adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=CountVectorizer()\n",
    "liste_vectorise=vect.fit_transform(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "024d7ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 14)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_vectorise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ea27c063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 9)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 0)\t1\n",
      "  (4, 0)\t1\n",
      "  (4, 7)\t1\n",
      "  (4, 5)\t1\n",
      "  (4, 13)\t1\n",
      "  (4, 10)\t1\n",
      "  (4, 3)\t1\n",
      "  (4, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "print(liste_vectorise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2eb4319f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11  12  13\n",
       "0   0   1   0   0   0   0   1   0   0   1   0   1   1   0\n",
       "1   0   1   0   0   0   0   0   0   1   0   0   0   0   0\n",
       "2   0   0   0   0   1   0   1   0   0   0   0   1   1   0\n",
       "3   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "4   1   0   1   1   0   1   0   1   0   0   1   0   0   1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix=pd.DataFrame(liste_vectorise.toarray())\n",
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75cd4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix.columns=vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e31c71b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>data</th>\n",
       "      <th>de</th>\n",
       "      <th>domaine</th>\n",
       "      <th>espritiens</th>\n",
       "      <th>est</th>\n",
       "      <th>je</th>\n",
       "      <th>ml</th>\n",
       "      <th>science</th>\n",
       "      <th>scientist</th>\n",
       "      <th>sous</th>\n",
       "      <th>suis</th>\n",
       "      <th>un</th>\n",
       "      <th>une</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  data  de  domaine  espritiens  est  je  ml  science  scientist  sous  \\\n",
       "0   0     1   0        0           0    0   1   0        0          1     0   \n",
       "1   0     1   0        0           0    0   0   0        1          0     0   \n",
       "2   0     0   0        0           1    0   1   0        0          0     0   \n",
       "3   1     0   0        0           0    0   0   0        0          0     0   \n",
       "4   1     0   1        1           0    1   0   1        0          0     1   \n",
       "\n",
       "   suis  un  une  \n",
       "0     1   1    0  \n",
       "1     0   0    0  \n",
       "2     1   1    0  \n",
       "3     0   0    0  \n",
       "4     0   0    1  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62359ba4",
   "metadata": {},
   "source": [
    "## vectorizing N-grames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4b48dbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>data</th>\n",
       "      <th>de</th>\n",
       "      <th>domaine</th>\n",
       "      <th>espritiens</th>\n",
       "      <th>est</th>\n",
       "      <th>je</th>\n",
       "      <th>ml</th>\n",
       "      <th>science</th>\n",
       "      <th>scientist</th>\n",
       "      <th>sous</th>\n",
       "      <th>suis</th>\n",
       "      <th>un</th>\n",
       "      <th>une</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  data  de  domaine  espritiens  est  je  ml  science  scientist  sous  \\\n",
       "0   0     1   0        0           0    0   1   0        0          1     0   \n",
       "1   0     1   0        0           0    0   0   0        1          0     0   \n",
       "2   0     0   0        0           1    0   1   0        0          0     0   \n",
       "3   1     0   0        0           0    0   0   0        0          0     0   \n",
       "4   1     0   1        1           0    1   0   1        0          0     1   \n",
       "\n",
       "   suis  un  une  \n",
       "0     1   1    0  \n",
       "1     0   0    0  \n",
       "2     1   1    0  \n",
       "3     0   0    0  \n",
       "4     0   0    1  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vect=CountVectorizer(ngram_range=(1,1))\n",
    "liste_ngram_vectorise=ngram_vect.fit_transform(l)\n",
    "sparse_matrix=pd.DataFrame(liste_ngram_vectorise.toarray())\n",
    "sparse_matrix.columns=ngram_vect.get_feature_names_out()\n",
    "sparse_matrix       # donc les memes resultats car on choisi le n_gram=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7f2bfe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',27)  # pour afficher tous les colones dans df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5bd61dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>data</th>\n",
       "      <th>data science</th>\n",
       "      <th>data scientist</th>\n",
       "      <th>de</th>\n",
       "      <th>de ai</th>\n",
       "      <th>domaine</th>\n",
       "      <th>domaine de</th>\n",
       "      <th>espritiens</th>\n",
       "      <th>est</th>\n",
       "      <th>est une</th>\n",
       "      <th>je</th>\n",
       "      <th>je suis</th>\n",
       "      <th>ml</th>\n",
       "      <th>ml est</th>\n",
       "      <th>science</th>\n",
       "      <th>scientist</th>\n",
       "      <th>sous</th>\n",
       "      <th>sous domaine</th>\n",
       "      <th>suis</th>\n",
       "      <th>suis un</th>\n",
       "      <th>un</th>\n",
       "      <th>un data</th>\n",
       "      <th>un espritiens</th>\n",
       "      <th>une</th>\n",
       "      <th>une sous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  data  data science  data scientist  de  de ai  domaine  domaine de  \\\n",
       "0   0     1             0               1   0      0        0           0   \n",
       "1   0     1             1               0   0      0        0           0   \n",
       "2   0     0             0               0   0      0        0           0   \n",
       "3   1     0             0               0   0      0        0           0   \n",
       "4   1     0             0               0   1      1        1           1   \n",
       "\n",
       "   espritiens  est  est une  je  je suis  ml  ml est  science  scientist  \\\n",
       "0           0    0        0   1        1   0       0        0          1   \n",
       "1           0    0        0   0        0   0       0        1          0   \n",
       "2           1    0        0   1        1   0       0        0          0   \n",
       "3           0    0        0   0        0   0       0        0          0   \n",
       "4           0    1        1   0        0   1       1        0          0   \n",
       "\n",
       "   sous  sous domaine  suis  suis un  un  un data  un espritiens  une  \\\n",
       "0     0             0     1        1   1        1              0    0   \n",
       "1     0             0     0        0   0        0              0    0   \n",
       "2     0             0     1        1   1        0              1    0   \n",
       "3     0             0     0        0   0        0              0    0   \n",
       "4     1             1     0        0   0        0              0    1   \n",
       "\n",
       "   une sous  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         1  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changeons maintenant le n_gram\n",
    "ngram_vect=CountVectorizer(ngram_range=(1,2))\n",
    "liste_ngram_vectorise=ngram_vect.fit_transform(l)\n",
    "sparse_matrix=pd.DataFrame(liste_ngram_vectorise.toarray())\n",
    "sparse_matrix.columns=ngram_vect.get_feature_names_out()\n",
    "sparse_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b266396f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ai', 'data', 'data science', 'data scientist', 'de', 'de ai',\n",
       "       'domaine', 'domaine de', 'espritiens', 'est', 'est une', 'je',\n",
       "       'je suis', 'ml', 'ml est', 'science', 'scientist', 'sous',\n",
       "       'sous domaine', 'suis', 'suis un', 'un', 'un data',\n",
       "       'un espritiens', 'une', 'une sous'], dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ebc37",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ddc38199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "96ecf1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>data</th>\n",
       "      <th>de</th>\n",
       "      <th>domaine</th>\n",
       "      <th>espritiens</th>\n",
       "      <th>est</th>\n",
       "      <th>je</th>\n",
       "      <th>ml</th>\n",
       "      <th>science</th>\n",
       "      <th>scientist</th>\n",
       "      <th>sous</th>\n",
       "      <th>suis</th>\n",
       "      <th>un</th>\n",
       "      <th>une</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.425001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425001</td>\n",
       "      <td>0.425001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.627914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.778283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.581951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469515</td>\n",
       "      <td>0.469515</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387757</td>\n",
       "      <td>0.387757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ai      data        de   domaine  espritiens       est        je  \\\n",
       "0  0.00000  0.425001  0.000000  0.000000    0.000000  0.000000  0.425001   \n",
       "1  0.00000  0.627914  0.000000  0.000000    0.000000  0.000000  0.000000   \n",
       "2  0.00000  0.000000  0.000000  0.000000    0.581951  0.000000  0.469515   \n",
       "3  1.00000  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000   \n",
       "4  0.31284  0.000000  0.387757  0.387757    0.000000  0.387757  0.000000   \n",
       "\n",
       "         ml   science  scientist      sous      suis        un       une  \n",
       "0  0.000000  0.000000   0.526778  0.000000  0.425001  0.425001  0.000000  \n",
       "1  0.000000  0.778283   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000   0.000000  0.000000  0.469515  0.469515  0.000000  \n",
       "3  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.387757  0.000000   0.000000  0.387757  0.000000  0.000000  0.387757  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf=TfidfVectorizer()\n",
    "liste_tfidf=tfidf.fit_transform(l)\n",
    "sparse_matrix=pd.DataFrame(liste_tfidf.toarray())\n",
    "sparse_matrix.columns=tfidf.get_feature_names_out()\n",
    "sparse_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3441015d",
   "metadata": {},
   "source": [
    "# APPLIQUONS LES MEME TECHNIQUES SUR NOTRE DATA df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bae666c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>02070836089</th>\n",
       "      <th>02072069400</th>\n",
       "      <th>02073162414</th>\n",
       "      <th>02085076972</th>\n",
       "      <th>...</th>\n",
       "      <th>åômorrow</th>\n",
       "      <th>åôrent</th>\n",
       "      <th>ìll</th>\n",
       "      <th>ìï</th>\n",
       "      <th>ìïll</th>\n",
       "      <th>ûthank</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªv</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharri</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 8034 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      008704050406  0089mi  0121  01223585236  01223585334  0125698789  02  \\\n",
       "0                0       0     0            0            0           0   0   \n",
       "1                0       0     0            0            0           0   0   \n",
       "2                0       0     0            0            0           0   0   \n",
       "3                0       0     0            0            0           0   0   \n",
       "4                0       0     0            0            0           0   0   \n",
       "...            ...     ...   ...          ...          ...         ...  ..   \n",
       "5567             0       0     0            0            0           0   0   \n",
       "5568             0       0     0            0            0           0   0   \n",
       "5569             0       0     0            0            0           0   0   \n",
       "5570             0       0     0            0            0           0   0   \n",
       "5571             0       0     0            0            0           0   0   \n",
       "\n",
       "      020603  0207  02070836089  02072069400  02073162414  02085076972  ...  \\\n",
       "0          0     0            0            0            0            0  ...   \n",
       "1          0     0            0            0            0            0  ...   \n",
       "2          0     0            0            0            0            0  ...   \n",
       "3          0     0            0            0            0            0  ...   \n",
       "4          0     0            0            0            0            0  ...   \n",
       "...      ...   ...          ...          ...          ...          ...  ...   \n",
       "5567       0     0            0            0            0            0  ...   \n",
       "5568       0     0            0            0            0            0  ...   \n",
       "5569       0     0            0            0            0            0  ...   \n",
       "5570       0     0            0            0            0            0  ...   \n",
       "5571       0     0            0            0            0            0  ...   \n",
       "\n",
       "      åômorrow  åôrent  ìll  ìï  ìïll  ûthank  ûªm  ûªt  ûªv  ûï  ûïharri  ûò  \\\n",
       "0            0       0    0   0     0       0    0    0    0   0        0   0   \n",
       "1            0       0    0   0     0       0    0    0    0   0        0   0   \n",
       "2            0       0    0   0     0       0    0    0    0   0        0   0   \n",
       "3            0       0    0   0     0       0    0    0    0   0        0   0   \n",
       "4            0       0    0   0     0       0    0    0    0   0        0   0   \n",
       "...        ...     ...  ...  ..   ...     ...  ...  ...  ...  ..      ...  ..   \n",
       "5567         0       0    0   0     0       0    0    0    0   0        0   0   \n",
       "5568         0       0    0   0     0       0    0    0    0   0        0   0   \n",
       "5569         0       0    0   0     0       0    0    0    0   0        0   0   \n",
       "5570         0       0    0   0     0       0    0    0    0   0        0   0   \n",
       "5571         0       0    0   0     0       0    0    0    0   0        0   0   \n",
       "\n",
       "      ûówel  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "5567      0  \n",
       "5568      0  \n",
       "5569      0  \n",
       "5570      0  \n",
       "5571      0  \n",
       "\n",
       "[5572 rows x 8034 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vectorise=vect.fit_transform(df['message_steming'])\n",
    "sparse_matrix=pd.DataFrame(df_vectorise.toarray())\n",
    "sparse_matrix.columns=vect.get_feature_names_out()\n",
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cbb143c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>008704050406</th>\n",
       "      <th>008704050406 sp</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0089mi last</th>\n",
       "      <th>0121</th>\n",
       "      <th>0121 2025050</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585236 xx</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>01223585334 cum</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>0125698789 ring</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>ûïharri potter</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûò address</th>\n",
       "      <th>ûò entertain</th>\n",
       "      <th>ûò even</th>\n",
       "      <th>ûò favour</th>\n",
       "      <th>ûò get</th>\n",
       "      <th>ûò hope</th>\n",
       "      <th>ûò inde</th>\n",
       "      <th>ûò limp</th>\n",
       "      <th>ûò sound</th>\n",
       "      <th>ûówel</th>\n",
       "      <th>ûówel done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 39196 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      008704050406  008704050406 sp  0089mi  0089mi last  0121  0121 2025050  \\\n",
       "0                0                0       0            0     0             0   \n",
       "1                0                0       0            0     0             0   \n",
       "2                0                0       0            0     0             0   \n",
       "3                0                0       0            0     0             0   \n",
       "4                0                0       0            0     0             0   \n",
       "...            ...              ...     ...          ...   ...           ...   \n",
       "5567             0                0       0            0     0             0   \n",
       "5568             0                0       0            0     0             0   \n",
       "5569             0                0       0            0     0             0   \n",
       "5570             0                0       0            0     0             0   \n",
       "5571             0                0       0            0     0             0   \n",
       "\n",
       "      01223585236  01223585236 xx  01223585334  01223585334 cum  0125698789  \\\n",
       "0               0               0            0                0           0   \n",
       "1               0               0            0                0           0   \n",
       "2               0               0            0                0           0   \n",
       "3               0               0            0                0           0   \n",
       "4               0               0            0                0           0   \n",
       "...           ...             ...          ...              ...         ...   \n",
       "5567            0               0            0                0           0   \n",
       "5568            0               0            0                0           0   \n",
       "5569            0               0            0                0           0   \n",
       "5570            0               0            0                0           0   \n",
       "5571            0               0            0                0           0   \n",
       "\n",
       "      0125698789 ring  02  ...  ûïharri potter  ûò  ûò address  ûò entertain  \\\n",
       "0                   0   0  ...               0   0           0             0   \n",
       "1                   0   0  ...               0   0           0             0   \n",
       "2                   0   0  ...               0   0           0             0   \n",
       "3                   0   0  ...               0   0           0             0   \n",
       "4                   0   0  ...               0   0           0             0   \n",
       "...               ...  ..  ...             ...  ..         ...           ...   \n",
       "5567                0   0  ...               0   0           0             0   \n",
       "5568                0   0  ...               0   0           0             0   \n",
       "5569                0   0  ...               0   0           0             0   \n",
       "5570                0   0  ...               0   0           0             0   \n",
       "5571                0   0  ...               0   0           0             0   \n",
       "\n",
       "      ûò even  ûò favour  ûò get  ûò hope  ûò inde  ûò limp  ûò sound  ûówel  \\\n",
       "0           0          0       0        0        0        0         0      0   \n",
       "1           0          0       0        0        0        0         0      0   \n",
       "2           0          0       0        0        0        0         0      0   \n",
       "3           0          0       0        0        0        0         0      0   \n",
       "4           0          0       0        0        0        0         0      0   \n",
       "...       ...        ...     ...      ...      ...      ...       ...    ...   \n",
       "5567        0          0       0        0        0        0         0      0   \n",
       "5568        0          0       0        0        0        0         0      0   \n",
       "5569        0          0       0        0        0        0         0      0   \n",
       "5570        0          0       0        0        0        0         0      0   \n",
       "5571        0          0       0        0        0        0         0      0   \n",
       "\n",
       "      ûówel done  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "5567           0  \n",
       "5568           0  \n",
       "5569           0  \n",
       "5570           0  \n",
       "5571           0  \n",
       "\n",
       "[5572 rows x 39196 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vect=CountVectorizer(ngram_range=(1,2))\n",
    "df_ngram_vectorise=ngram_vect.fit_transform(df['message_steming'])\n",
    "sparse_matrix=pd.DataFrame(df_ngram_vectorise.toarray())\n",
    "sparse_matrix.columns=ngram_vect.get_feature_names_out()\n",
    "sparse_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7811aa08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>02070836089</th>\n",
       "      <th>02072069400</th>\n",
       "      <th>02073162414</th>\n",
       "      <th>02085076972</th>\n",
       "      <th>...</th>\n",
       "      <th>åômorrow</th>\n",
       "      <th>åôrent</th>\n",
       "      <th>ìll</th>\n",
       "      <th>ìï</th>\n",
       "      <th>ìïll</th>\n",
       "      <th>ûthank</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªv</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharri</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 8034 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      008704050406  0089mi  0121  01223585236  01223585334  0125698789   02  \\\n",
       "0              0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "1              0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "2              0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "3              0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "4              0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "...            ...     ...   ...          ...          ...         ...  ...   \n",
       "5567           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "5568           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "5569           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "5570           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "5571           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "\n",
       "      020603  0207  02070836089  02072069400  02073162414  02085076972  ...  \\\n",
       "0        0.0   0.0          0.0          0.0          0.0          0.0  ...   \n",
       "1        0.0   0.0          0.0          0.0          0.0          0.0  ...   \n",
       "2        0.0   0.0          0.0          0.0          0.0          0.0  ...   \n",
       "3        0.0   0.0          0.0          0.0          0.0          0.0  ...   \n",
       "4        0.0   0.0          0.0          0.0          0.0          0.0  ...   \n",
       "...      ...   ...          ...          ...          ...          ...  ...   \n",
       "5567     0.0   0.0          0.0          0.0          0.0          0.0  ...   \n",
       "5568     0.0   0.0          0.0          0.0          0.0          0.0  ...   \n",
       "5569     0.0   0.0          0.0          0.0          0.0          0.0  ...   \n",
       "5570     0.0   0.0          0.0          0.0          0.0          0.0  ...   \n",
       "5571     0.0   0.0          0.0          0.0          0.0          0.0  ...   \n",
       "\n",
       "      åômorrow  åôrent  ìll   ìï  ìïll  ûthank  ûªm  ûªt  ûªv   ûï  ûïharri  \\\n",
       "0          0.0     0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0      0.0   \n",
       "1          0.0     0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0      0.0   \n",
       "2          0.0     0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0      0.0   \n",
       "3          0.0     0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0      0.0   \n",
       "4          0.0     0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0      0.0   \n",
       "...        ...     ...  ...  ...   ...     ...  ...  ...  ...  ...      ...   \n",
       "5567       0.0     0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0      0.0   \n",
       "5568       0.0     0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0      0.0   \n",
       "5569       0.0     0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0      0.0   \n",
       "5570       0.0     0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0      0.0   \n",
       "5571       0.0     0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0      0.0   \n",
       "\n",
       "       ûò  ûówel  \n",
       "0     0.0    0.0  \n",
       "1     0.0    0.0  \n",
       "2     0.0    0.0  \n",
       "3     0.0    0.0  \n",
       "4     0.0    0.0  \n",
       "...   ...    ...  \n",
       "5567  0.0    0.0  \n",
       "5568  0.0    0.0  \n",
       "5569  0.0    0.0  \n",
       "5570  0.0    0.0  \n",
       "5571  0.0    0.0  \n",
       "\n",
       "[5572 rows x 8034 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf=tfidf.fit_transform(df['message_steming'])\n",
    "sparse_matrixtfidf=pd.DataFrame(df_tfidf.toarray())\n",
    "sparse_matrixtfidf.columns=tfidf.get_feature_names_out()\n",
    "sparse_matrixtfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a8f34d",
   "metadata": {},
   "source": [
    "# FEATURES-ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c3416",
   "metadata": {},
   "source": [
    "## posons ds hypotheses et tester ces leurs verites \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "05c5f0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_de_message</th>\n",
       "      <th>le_message</th>\n",
       "      <th>message_steming</th>\n",
       "      <th>longueur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry light turned green, I meant another friend wanted  &amp;lt;#&amp;gt;  worth but he may not be around</td>\n",
       "      <td>sorri light turn green i meant anoth friend want ltgt worth may around</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4408</th>\n",
       "      <td>spam</td>\n",
       "      <td>For your chance to WIN a FREE Bluetooth Headset then simply reply back with \\ADP\\\"\"</td>\n",
       "      <td>chanc win a free bluetooth headset simpli repli back adp</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>ham</td>\n",
       "      <td>Then u going ikea str aft dat?</td>\n",
       "      <td>u go ikea str aft dat</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type_de_message  \\\n",
       "828              ham   \n",
       "4408            spam   \n",
       "1909             ham   \n",
       "\n",
       "                                                                                              le_message  \\\n",
       "828   Sorry light turned green, I meant another friend wanted  &lt;#&gt;  worth but he may not be around   \n",
       "4408                 For your chance to WIN a FREE Bluetooth Headset then simply reply back with \\ADP\\\"\"   \n",
       "1909                                                                      Then u going ikea str aft dat?   \n",
       "\n",
       "                                                             message_steming  \\\n",
       "828   sorri light turn green i meant anoth friend want ltgt worth may around   \n",
       "4408                chanc win a free bluetooth headset simpli repli back adp   \n",
       "1909                                                   u go ikea str aft dat   \n",
       "\n",
       "      longueur  \n",
       "828         80  \n",
       "4408        69  \n",
       "1909        24  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HYPOTHESE 1 : en generales les messges tres long ou tres courts sont des spams\n",
    "# cree une colone qui contient le longueur du message\n",
    "df['longueur']=df['le_message'].apply( lambda x:len(x)-x.count(' '))\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8fd91354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   4,   4,   4,   5,   5,   5,   5,   6,\n",
       "         6,   6,   6,   7,   7,   8,   8,   9,   9,  10,  11,  11,  12,\n",
       "        12,  12,  13,  13,  13,  13,  14,  14,  14,  15,  16,  17,  17,\n",
       "        17,  17,  18,  18,  18,  18,  18,  19,  19,  19,  19,  19,  20,\n",
       "        20,  20,  21,  21,  22,  22,  22,  22,  22,  22,  23,  23,  23,\n",
       "        23,  23,  23,  24,  24,  25,  25,  26,  27,  27,  27,  27,  28,\n",
       "        29,  30,  31,  31,  31,  31,  31,  31,  31,  32,  32,  32,  33,\n",
       "        34,  34,  35,  35,  35,  35,  35,  36,  36,  36,  37,  37,  37,\n",
       "        38,  38,  38,  40,  41,  43,  43,  43,  43,  44,  44,  44,  44,\n",
       "        45,  47,  47,  48,  49,  51,  51,  52,  53,  55,  55,  56,  59,\n",
       "        62,  70,  71,  77,  78,  79,  79,  81,  83,  83,  84,  85,  86,\n",
       "        88,  96,  97, 101, 101, 102, 102, 113, 118, 121, 128], dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "les_longueurs=df.longueur.value_counts().values\n",
    "les_longueurs.sort()\n",
    "les_longueurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3d45e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPOTHESE 2 : les mesaages qui contient plusieurs signes de ponctuation sont des spam\n",
    "# creeons une colone qui contient les les nombre de signes de ponctuation\n",
    "# definisson une fonction qui calcule le nombre des signes de ponctuation\n",
    "def calcul_ponc(text):\n",
    "    s=0\n",
    "    lon=len(text)\n",
    "    for x in text:\n",
    "        if x in str.punctuation:\n",
    "            s+=1\n",
    "    return s/lon   # biensur le \n",
    "    \n",
    "df['lon_ponc']=df['le_message'].apply(lambda x: calcul_ponc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "769ac353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_de_message</th>\n",
       "      <th>le_message</th>\n",
       "      <th>message_steming</th>\n",
       "      <th>longueur</th>\n",
       "      <th>lon_ponc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yo, you at jp and hungry like a mofo?</td>\n",
       "      <td>yo jp hungri like a mofo</td>\n",
       "      <td>29</td>\n",
       "      <td>0.054054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5322</th>\n",
       "      <td>ham</td>\n",
       "      <td>Thanks, I'll keep that in mind</td>\n",
       "      <td>thank ill keep mind</td>\n",
       "      <td>25</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>ham</td>\n",
       "      <td>Beautiful Truth against Gravity.. Read carefully: \\Our heart feels light when someone is in it.....</td>\n",
       "      <td>beauti truth graviti read care heart feel light someon feel heavi someon leav good night</td>\n",
       "      <td>134</td>\n",
       "      <td>0.069182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4883</th>\n",
       "      <td>ham</td>\n",
       "      <td>For many things its an antibiotic and it can be used for chest abdomen and gynae infections even...</td>\n",
       "      <td>mani thing antibiot use chest abdomen gyna infect even bone infect</td>\n",
       "      <td>94</td>\n",
       "      <td>0.008850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type_de_message  \\\n",
       "3096             ham   \n",
       "5322             ham   \n",
       "2124             ham   \n",
       "4883             ham   \n",
       "\n",
       "                                                                                               le_message  \\\n",
       "3096                                                                Yo, you at jp and hungry like a mofo?   \n",
       "5322                                                                       Thanks, I'll keep that in mind   \n",
       "2124  Beautiful Truth against Gravity.. Read carefully: \\Our heart feels light when someone is in it.....   \n",
       "4883  For many things its an antibiotic and it can be used for chest abdomen and gynae infections even...   \n",
       "\n",
       "                                                                               message_steming  \\\n",
       "3096                                                                  yo jp hungri like a mofo   \n",
       "5322                                                                       thank ill keep mind   \n",
       "2124  beauti truth graviti read care heart feel light someon feel heavi someon leav good night   \n",
       "4883                        mani thing antibiot use chest abdomen gyna infect even bone infect   \n",
       "\n",
       "      longueur  lon_ponc  \n",
       "3096        29  0.054054  \n",
       "5322        25  0.066667  \n",
       "2124       134  0.069182  \n",
       "4883        94  0.008850  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "953c827c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lon_ponc\n",
       "0.000000    468\n",
       "0.090909     78\n",
       "0.043478     71\n",
       "0.041667     68\n",
       "0.045455     63\n",
       "           ... \n",
       "0.030864      1\n",
       "0.028796      1\n",
       "0.051095      1\n",
       "0.016216      1\n",
       "0.008000      1\n",
       "Name: count, Length: 994, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.lon_ponc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a70cfc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPOTHESE 3 : les messages spam contient plusieurs letre majuscules\n",
    "def count_maj(text):\n",
    "    lon=len(text)\n",
    "    s=0\n",
    "    for x in text:\n",
    "        if x.isupper():\n",
    "            s+=1\n",
    "    return s/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "da3ce9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nbr_maj']=df['le_message'].apply(lambda x: count_maj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ffd303b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_de_message</th>\n",
       "      <th>le_message</th>\n",
       "      <th>message_steming</th>\n",
       "      <th>longueur</th>\n",
       "      <th>lon_ponc</th>\n",
       "      <th>nbr_maj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fine i miss you very much.</td>\n",
       "      <td>fine i miss much</td>\n",
       "      <td>21</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yoyyooo u know how to change permissions for a drive in mac. My usb flash drive</td>\n",
       "      <td>yoyyooo u know chang permiss a drive mac usb flash drive</td>\n",
       "      <td>64</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.025316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type_de_message  \\\n",
       "653             ham   \n",
       "884             ham   \n",
       "\n",
       "                                                                          le_message  \\\n",
       "653                                                       Fine i miss you very much.   \n",
       "884  Yoyyooo u know how to change permissions for a drive in mac. My usb flash drive   \n",
       "\n",
       "                                              message_steming  longueur  \\\n",
       "653                                          fine i miss much        21   \n",
       "884  yoyyooo u know chang permiss a drive mac usb flash drive        64   \n",
       "\n",
       "     lon_ponc   nbr_maj  \n",
       "653  0.038462  0.038462  \n",
       "884  0.012658  0.025316  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9e9cc9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longueur</th>\n",
       "      <th>lon_ponc</th>\n",
       "      <th>nbr_maj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>65.512024</td>\n",
       "      <td>0.059797</td>\n",
       "      <td>0.066374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>48.629795</td>\n",
       "      <td>0.058587</td>\n",
       "      <td>0.109094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.025974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>98.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>740.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longueur     lon_ponc      nbr_maj\n",
       "count  5572.000000  5572.000000  5572.000000\n",
       "mean     65.512024     0.059797     0.066374\n",
       "std      48.629795     0.058587     0.109094\n",
       "min       2.000000     0.000000     0.000000\n",
       "25%      29.000000     0.026846     0.025974\n",
       "50%      50.000000     0.045455     0.038462\n",
       "75%      98.000000     0.076923     0.062500\n",
       "max     740.000000     0.857143     1.000000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6749e1",
   "metadata": {},
   "source": [
    "# RESCALING THE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bcb09f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ad0f5cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "27956dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['nbr_maj','lon_ponc','longueur']]=scaler.fit_transform(df[['nbr_maj','lon_ponc','longueur']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f869b4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_de_message</th>\n",
       "      <th>le_message</th>\n",
       "      <th>message_steming</th>\n",
       "      <th>longueur</th>\n",
       "      <th>lon_ponc</th>\n",
       "      <th>nbr_maj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4409</th>\n",
       "      <td>ham</td>\n",
       "      <td>You also didnt get na hi hi hi hi hi</td>\n",
       "      <td>also didnt get na hi hi hi hi hi</td>\n",
       "      <td>0.033875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>ham</td>\n",
       "      <td>How come guoyang go n tell her? Then u told her?</td>\n",
       "      <td>come guoyang go n tell u told</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.048611</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5153</th>\n",
       "      <td>ham</td>\n",
       "      <td>Haven't left yet so probably gonna be here til dinner</td>\n",
       "      <td>havent left yet probabl gon na til dinner</td>\n",
       "      <td>0.056911</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.018868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type_de_message                                             le_message  \\\n",
       "4409             ham                   You also didnt get na hi hi hi hi hi   \n",
       "3933             ham       How come guoyang go n tell her? Then u told her?   \n",
       "5153             ham  Haven't left yet so probably gonna be here til dinner   \n",
       "\n",
       "                                message_steming  longueur  lon_ponc   nbr_maj  \n",
       "4409           also didnt get na hi hi hi hi hi  0.033875  0.000000  0.027778  \n",
       "3933              come guoyang go n tell u told  0.048780  0.048611  0.041667  \n",
       "5153  havent left yet probabl gon na til dinner  0.056911  0.022013  0.018868  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "12e09c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 6)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dea7b0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 8034)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrixtfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "728d47aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>02070836089</th>\n",
       "      <th>02072069400</th>\n",
       "      <th>02073162414</th>\n",
       "      <th>02085076972</th>\n",
       "      <th>...</th>\n",
       "      <th>åômorrow</th>\n",
       "      <th>åôrent</th>\n",
       "      <th>ìll</th>\n",
       "      <th>ìï</th>\n",
       "      <th>ìïll</th>\n",
       "      <th>ûthank</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªv</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharri</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4473</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4139</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.236311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 8034 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      008704050406  0089mi  0121  01223585236  01223585334  0125698789   02  \\\n",
       "4473           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "4139           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "373            0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "\n",
       "      020603  0207  02070836089  02072069400  02073162414  02085076972  ...  \\\n",
       "4473     0.0   0.0          0.0          0.0     0.386212          0.0  ...   \n",
       "4139     0.0   0.0          0.0          0.0     0.000000          0.0  ...   \n",
       "373      0.0   0.0          0.0          0.0     0.000000          0.0  ...   \n",
       "\n",
       "      åômorrow  åôrent  ìll        ìï  ìïll  ûthank  ûªm  ûªt  ûªv   ûï  \\\n",
       "4473       0.0     0.0  0.0  0.000000   0.0     0.0  0.0  0.0  0.0  0.0   \n",
       "4139       0.0     0.0  0.0  0.236311   0.0     0.0  0.0  0.0  0.0  0.0   \n",
       "373        0.0     0.0  0.0  0.000000   0.0     0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      ûïharri   ûò  ûówel  \n",
       "4473      0.0  0.0    0.0  \n",
       "4139      0.0  0.0    0.0  \n",
       "373       0.0  0.0    0.0  \n",
       "\n",
       "[3 rows x 8034 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrixtfidf.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6101621",
   "metadata": {},
   "source": [
    "# MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "038c593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.concat([df['lon_ponc'],df['nbr_maj'],df['longueur'],sparse_matrixtfidf],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "396355df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon_ponc</th>\n",
       "      <th>nbr_maj</th>\n",
       "      <th>longueur</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>02070836089</th>\n",
       "      <th>...</th>\n",
       "      <th>åômorrow</th>\n",
       "      <th>åôrent</th>\n",
       "      <th>ìll</th>\n",
       "      <th>ìï</th>\n",
       "      <th>ìïll</th>\n",
       "      <th>ûthank</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªv</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharri</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5038</th>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.102981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.028455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>0.131720</td>\n",
       "      <td>0.112903</td>\n",
       "      <td>0.070461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 8037 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lon_ponc   nbr_maj  longueur  008704050406  0089mi  0121  01223585236  \\\n",
       "5038  0.011905  0.010204  0.102981           0.0     0.0   0.0          0.0   \n",
       "196   0.000000  0.035714  0.028455           0.0     0.0   0.0          0.0   \n",
       "892   0.131720  0.112903  0.070461           0.0     0.0   0.0          0.0   \n",
       "\n",
       "      01223585334  0125698789   02  020603  0207  02070836089  ...  åômorrow  \\\n",
       "5038          0.0         0.0  0.0     0.0   0.0          0.0  ...       0.0   \n",
       "196           0.0         0.0  0.0     0.0   0.0          0.0  ...       0.0   \n",
       "892           0.0         0.0  0.0     0.0   0.0          0.0  ...       0.0   \n",
       "\n",
       "      åôrent  ìll   ìï  ìïll  ûthank  ûªm  ûªt  ûªv   ûï  ûïharri   ûò  ûówel  \n",
       "5038     0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0      0.0  0.0    0.0  \n",
       "196      0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0      0.0  0.0    0.0  \n",
       "892      0.0  0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0      0.0  0.0    0.0  \n",
       "\n",
       "[3 rows x 8037 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f1303098",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df['type_de_message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9e415315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4122     ham\n",
       "2327    spam\n",
       "2734     ham\n",
       "Name: type_de_message, dtype: object"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca2fb8f",
   "metadata": {},
   "source": [
    "### petit definission de notre algorithme utiliser RANDOMFOREST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3a942",
   "metadata": {},
   "source": [
    "Le Random Forest (forêt aléatoire) est un algorithme d'apprentissage automatique utilisé pour la classification et la régression. Il fonctionne en combinant plusieurs arbres de décision, chacun étant entraîné sur un échantillon aléatoire des données d'entraînement. Les prédictions des arbres sont ensuite combinées par vote majoritaire (classification) ou moyenne (régression).\n",
    "\n",
    "Points Clés\n",
    "Ensemble de modèles : Utilise plusieurs arbres de décision pour améliorer précision et robustesse.\n",
    "Bagging : Les arbres sont entraînés sur des échantillons aléatoires avec remise, réduisant la variance.\n",
    "Sélection aléatoire des features : Chaque nœud d'arbre utilise un sous-ensemble aléatoire de features, augmentant la diversité.\n",
    "Avantages\n",
    "Réduction du surapprentissage : Moins sujet au surapprentissage par rapport à un arbre unique.\n",
    "Haute précision : Souvent très précis avec peu de réglages de paramètres.\n",
    "Flexibilité : Applicable à la classification et à la régression.\n",
    "Inconvénients\n",
    "Complexité : Exigeant en calcul et en mémoire.\n",
    "Interprétabilité réduite : Difficile à interpréter en raison des nombreux arbres.\n",
    "Applications\n",
    "Diagnostic médical\n",
    "Finance\n",
    "Marketing\n",
    "Le Random Forest est une technique puissante et flexible, offrant une bonne précision et résistant au surapprentissage, malgré une interprétabilité limitée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c90de3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8dcd01bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4346, 8037), (1226, 8037), (4346,), (1226,))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=.22)\n",
    "X_train.shape,X_test.shape ,Y_train.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7654683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7ff86607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestClassifier(n_jobs=-1,n_estimators=100)  # n_jobs accelere l'execution au lieu d'etre sequentiel elle sera simultanement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8e193265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__sklearn_clone__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_build_request_for_signature',\n",
       " '_check_feature_names',\n",
       " '_check_n_features',\n",
       " '_compute_oob_predictions',\n",
       " '_doc_link_module',\n",
       " '_doc_link_template',\n",
       " '_doc_link_url_param_generator',\n",
       " '_estimator_type',\n",
       " '_get_default_requests',\n",
       " '_get_doc_link',\n",
       " '_get_estimators_indices',\n",
       " '_get_metadata_request',\n",
       " '_get_oob_predictions',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_make_estimator',\n",
       " '_more_tags',\n",
       " '_parameter_constraints',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_required_parameters',\n",
       " '_set_oob_score_and_attributes',\n",
       " '_validate_X_predict',\n",
       " '_validate_data',\n",
       " '_validate_estimator',\n",
       " '_validate_params',\n",
       " '_validate_y_class_weight',\n",
       " 'apply',\n",
       " 'decision_path',\n",
       " 'estimators_samples_',\n",
       " 'feature_importances_',\n",
       " 'fit',\n",
       " 'get_metadata_routing',\n",
       " 'get_params',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'score',\n",
       " 'set_fit_request',\n",
       " 'set_params',\n",
       " 'set_score_request']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4ccb7f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_jobs=-1)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "be05933c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9690048939641109"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850071f4",
   "metadata": {},
   "source": [
    "## chexking the importances features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6ac8b3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.04645547746039599, 'longueur'),\n",
       " (0.03705510924216261, 'txt'),\n",
       " (0.03649356619079401, 'nbr_maj'),\n",
       " (0.026566527925146573, 'call'),\n",
       " (0.01910805711334825, 'mobil'),\n",
       " (0.017312911785262654, 'claim'),\n",
       " (0.01689745387755963, 'servic'),\n",
       " (0.015707766306149465, 'free'),\n",
       " (0.014504392560086508, 'repli'),\n",
       " (0.013616498549869205, 'text'),\n",
       " (0.012586372664246948, 'stop'),\n",
       " (0.01159164405482996, '500'),\n",
       " (0.010662324234391922, 'prize'),\n",
       " (0.009541291425362634, 'lon_ponc'),\n",
       " (0.009174850827780212, 'win')]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(model.feature_importances_,X_train.columns),reverse=True)[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c9c86",
   "metadata": {},
   "source": [
    "# MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696574e",
   "metadata": {},
   "source": [
    "## utiliser le randomforestclassifier avec cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f292e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold ,cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d5b3bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0b5c6947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97586207, 0.9712313 , 0.97698504, 0.97583429, 0.9735328 ])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model,X_train,Y_train,cv=kf,scoring='accuracy',n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8386c5ee",
   "metadata": {},
   "source": [
    "## accuracy et confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da3355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=model.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "be6020ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "47525dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1062,    0],\n",
       "       [   7,  157]], dtype=int64)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred=model.predict(X_test)\n",
    "con_mat=confusion_matrix(Y_test,Y_pred)\n",
    "con_mat  # attetion en sckit-learn la confusion matrix est different en les srs sur net  est classe comme ci dessus\n",
    "# TN        FP\n",
    "# FN        TP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
